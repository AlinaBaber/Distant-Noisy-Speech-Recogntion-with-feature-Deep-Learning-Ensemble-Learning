{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj0mutNEz4VT"
      },
      "source": [
        "### Speech Processing Analysis & Machine Learning & Ensemble Modelling\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXCBp_7hPapa",
        "outputId": "be2d63c1-8152-4b0d-ee25-efec1c855f3d"
      },
      "source": [
        "!pip install spafe\n",
        "!pip install pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.1.2-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.19.5)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.1.2\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot, install\n",
            "Successfully installed install-1.3.4 scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPWozanHPX7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565ec7fd-6d2e-45f9-a226-151933b50a25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUxJhtRmt3fq",
        "outputId": "3970e166-713d-4744-e05d-0b131488e14d"
      },
      "source": [
        "import os\n",
        "for i in os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset/\"):\n",
        "    print(str(len(os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset/\"+i))) +\" \"+ i +\" audios\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1356 bed audios\n",
            "1357 bird audios\n",
            "1424 cat audios\n",
            "1198 down audios\n",
            "1498 dog audios\n",
            "1133 eight audios\n",
            "1092 five audios\n",
            "960 go audios\n",
            "2400 four audios\n",
            "1481 happy audios\n",
            "1505 left audios\n",
            "2392 house audios\n",
            "1253 marvel audios\n",
            "1144 nine audios\n",
            "1002 no audios\n",
            "2244 off audios\n",
            "2228 on audios\n",
            "1276 one audios\n",
            "1296 right audios\n",
            "1411 seven audios\n",
            "1485 six audios\n",
            "1463 sheila audios\n",
            "1188 three audios\n",
            "1485 stop audios\n",
            "1188 tree audios\n",
            "902 two audios\n",
            "1187 up audios\n",
            "957 wow audios\n",
            "1244 yes audios\n",
            "1306 zero audios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFvUbGcKvkhX",
        "outputId": "f4831381-1fc4-4a1c-a2ee-8b9cb96ebd06"
      },
      "source": [
        "import os\n",
        "for i in os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\"):\n",
        "    print(str(len(os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\"+i))) +\" \"+ i +\" audios\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1489 dog audios\n",
            "1402 cat audios\n",
            "1113 eight audios\n",
            "1223 down audios\n",
            "1112 five audios\n",
            "960 go audios\n",
            "2400 four audios\n",
            "1481 happy audios\n",
            "2392 house audios\n",
            "1485 left audios\n",
            "1253 marvel audios\n",
            "1145 nine audios\n",
            "963 no audios\n",
            "2252 off audios\n",
            "2228 on audios\n",
            "1276 one audios\n",
            "1276 right audios\n",
            "1411 seven audios\n",
            "1463 sheila audios\n",
            "1500 six audios\n",
            "1485 stop audios\n",
            "2028 three audios\n",
            "2072 tree audios\n",
            "902 two audios\n",
            "1187 up audios\n",
            "957 wow audios\n",
            "1547 yes audios\n",
            "1602 zero audios\n",
            "1367 bed audios\n",
            "1366 bird audios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhOZUt1vx9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b477af40-6310-46f6-e241-3c80669cc2e7"
      },
      "source": [
        "import os\n",
        "for i in os.listdir(\"/content/drive/MyDrive/speech_commands_v0.01/\"):\n",
        "    print(str(len(os.listdir(\"/content/drive/MyDrive/speech_commands_v0.01/\"+i))) +\" \"+ i +\" audios\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2357 five audios\n",
            "2352 eight audios\n",
            "2372 four audios\n",
            "2372 go audios\n",
            "1742 happy audios\n",
            "1750 house audios\n",
            "2353 left audios\n",
            "1746 marvin audios\n",
            "2365 nine audios\n",
            "1881 no audios\n",
            "2357 off audios\n",
            "2392 on audios\n",
            "2370 one audios\n",
            "2367 right audios\n",
            "2392 seven audios\n",
            "1734 sheila audios\n",
            "2387 six audios\n",
            "2390 stop audios\n",
            "2346 three audios\n",
            "1733 tree audios\n",
            "2373 two audios\n",
            "1745 wow audios\n",
            "2387 yes audios\n",
            "2376 up audios\n",
            "2376 zero audios\n",
            "1713 bed audios\n",
            "1731 bird audios\n",
            "1733 cat audios\n",
            "2359 down audios\n",
            "1746 dog audios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PNwFRsj0a-S"
      },
      "source": [
        "###  Our User defined Rebust ensemble Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr5_22APBLjI"
      },
      "source": [
        "import glob\n",
        "import shutil\n",
        "from collections import Counter\n",
        "import librosa.display\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import warnings\n",
        "import joblib\n",
        "from features import *\n",
        "import os\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import numpy as np\n",
        "import librosa\n",
        "import copy\n",
        "from AudioAnalyzer import SpectrumCompare\n",
        "\n",
        "def speechmodels(sample):\n",
        "    warnings.filterwarnings('ignore')\n",
        "    # for Machine Learning with random Forest\n",
        "\n",
        "    # load, extract, and visualize the features\n",
        "    features = process_data_for_ML_Rf(sample)\n",
        "    # prediction\n",
        "    pred_ML_RF = predict_ML_RF(features)\n",
        "    pred_ML_KNN = predict_ML_KNN(features)\n",
        "    pred_ML_SVM= predict_ML_SVM(features)\n",
        "    pred_ML_Vote= predict_ML_VOTE(features)\n",
        "   # Votes=pred_ML_RF,pred_ML_KNN,pred_ML_SVM,pred_ML_Vote\n",
        "    return pred_ML_RF,pred_ML_KNN,pred_ML_SVM,pred_ML_Vote\n",
        "\n",
        "# for Machine Learning on KNN with Features\n",
        "def predict_ML_KNN(features):\n",
        "    loaded_model = joblib.load('content/model_knn.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "def predict_ML_SVM(features):\n",
        "    loaded_model = joblib.load('content/model_svm.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "def predict_ML_VOTE(features):\n",
        "    loaded_model = joblib.load('content/model_voting.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "\n",
        "def predict_CNN(features):\n",
        "    loaded_model = joblib.load('content/model_knnwords.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "def predict_CNN_FE(features):\n",
        "    loaded_model = joblib.load('content/model_knnwords.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "def predict_LSTM(features):\n",
        "    loaded_model = joblib.load('content/model_knnwords.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "def predict_LSTM(features):\n",
        "    loaded_model = joblib.load('content/model_knnwords.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "def predict_LSTM_FE(features):\n",
        "    loaded_model = joblib.load('content/model_knnwords.sav')\n",
        "\n",
        "    pred = loaded_model.predict(features.reshape(1, 6000))\n",
        "\n",
        "    return pred\n",
        "\n",
        "class feature_analysis_graphs():\n",
        "    def sample_graph(self, samples, sample_rate):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        librosa.display.waveplot(samples, sr=sample_rate)\n",
        "        ax.label_outer()\n",
        "        ax.set(title='Data Respresentation')\n",
        "        plt.show()\n",
        "\n",
        "    def MFCC_graph(self, samples):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(samples, x_axis='time', ax=ax)\n",
        "        ax.set(title='MFCC')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def melspectrogram_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        S_dB = librosa.power_to_db(data, ref=np.max)\n",
        "        img = librosa.display.specshow(S_dB, x_axis='time',\n",
        "                                       y_axis='mel', sr=16000,\n",
        "                                       fmax=8000, ax=ax)\n",
        "        ax.set(title='Mel-frequency spectrogram')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def poly_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data[1].T, alpha=0.8, label='Poly Feature')\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def zero_crossing_rate_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data[0], label='zero crossing rate')\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def process_data_for_FE(samples):\n",
        "    sample_rate = 16000\n",
        "    #graph=feature_analysis_graphs()\n",
        "    #graph.sample_graph(samples, sample_rate)\n",
        "    # Extract Feautures\n",
        "    MFCC = mfcc_feature(samples, sample_rate)\n",
        "    #graph.MFCC_graph(MFCC)\n",
        "    MSS = melspectrogram_feature(samples, sample_rate)\n",
        "    #graph.melspectrogram_graph(MSS)\n",
        "    poly = poly_feature(samples, sample_rate)\n",
        "    #graph.poly_graph(poly)\n",
        "    ZCR = zero_crossing_rate_features(samples)\n",
        "    #graph.zero_crossing_rate_graph(ZCR)\n",
        "\n",
        "    # flatten an array\n",
        "    MFCC = MFCC.flatten()\n",
        "    MSS = MSS.flatten()\n",
        "    poly = poly.flatten()\n",
        "    ZCR = ZCR.flatten()\n",
        "\n",
        "    # adding features into single array\n",
        "    features = np.concatenate((MFCC, MSS, poly, ZCR))\n",
        "\n",
        "    # padding and trimming\n",
        "    max_len = 6000\n",
        "\n",
        "    pad_width = max_len - features.shape[0]\n",
        "    if pad_width > 0:\n",
        "        features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "\n",
        "    features = features[:max_len]\n",
        "\n",
        "    return features\n",
        "# for Deep Learningwih features\n",
        "\n",
        "\n",
        "def normalize_2d(v):\n",
        "    for i in range(v.shape[0]):\n",
        "        norm = np.linalg.norm(v[i])\n",
        "        if norm == 0:\n",
        "            v[i] = v[i]\n",
        "        else:\n",
        "            v[i] = v[i] / norm\n",
        "    return v\n",
        "    # adjust target amplitude\n",
        "def match_target_amplitude(sound, target_dBFS):\n",
        "    change_in_dBFS = target_dBFS - sound.dBFS\n",
        "    return sound.apply_gain(change_in_dBFS)\n",
        "\n",
        "def makechunk(Inputfilename, sample_rate, foldername):\n",
        "\n",
        "    #os.remove(foldername)\n",
        "    sound_file = AudioSegment.from_wav(Inputfilename)\n",
        "    audio_chunks = split_on_silence(sound_file, min_silence_len=40, silence_thresh=-36)\n",
        "    # os.mkdir('content')\n",
        "    os.mkdir(foldername)\n",
        "    for i, chunk in enumerate(audio_chunks):\n",
        "        out_file = foldername+\"/chunk{0}.wav\".format(i)\n",
        "        print(\"exporting\", out_file)\n",
        "        chunk.export(out_file, format=\"wav\")\n",
        "    sound_files = []\n",
        "    chunks = os.listdir(foldername)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk = foldername + \"/\" + chunk\n",
        "        print(chunk)\n",
        "        sample, sample_rate = librosa.load(chunk, sr=sample_rate)\n",
        "        sound_files.append(sample)\n",
        "    from pydub.silence import detect_nonsilent\n",
        "\n",
        "    # Convert wav to audio_segment\n",
        "    audio_segment = AudioSegment.from_wav(Inputfilename)\n",
        "\n",
        "    # normalize audio_segment to -20dBFS\n",
        "    normalized_sound = match_target_amplitude(audio_segment, -20.0)\n",
        "    print(\"length of audio_segment={} seconds\".format(len(normalized_sound) / 1000))\n",
        "\n",
        "    # Print detected non-silent chunks, which in our case would be spoken words.\n",
        "    nonsilent_data = detect_nonsilent(normalized_sound, min_silence_len=40, silence_thresh=-36, seek_step=1)\n",
        "    # convert ms to seconds\n",
        "    print(\"start,Stop\")\n",
        "    chunks_timestamps=list()\n",
        "    for chunks in nonsilent_data:\n",
        "        chunks_timestamps.append([chunk / 1000 for chunk in chunks])\n",
        "        print([chunk / 1000 for chunk in chunks])\n",
        "    return chunks_timestamps\n",
        "\n",
        "def Main(Inputfilename):\n",
        "    folder = 'chunks'\n",
        "\n",
        "    try:\n",
        "        for dir in os.listdir(folder):\n",
        "            shutil.rmtree(os.path.join(folder, dir))\n",
        "    except OSError as e:\n",
        "        print(\"Error: %s : %s\" % (folder, e.strerror))\n",
        "    #Inputfilename = \"Al-Kawthar/Al-Kawthar_001/108001_06.wav\"\n",
        "    sample_rate = 16000\n",
        "    Predictedfoldername = \"chunks/PredictedTest\"\n",
        "    chunks_timestamps=makechunk(Inputfilename, sample_rate, Predictedfoldername)\n",
        "    test_chunks = []\n",
        "    for filename in glob.glob(os.path.join(Predictedfoldername, '*.wav')):\n",
        "        test_chunks.append(filename)\n",
        "    print(test_chunks)\n",
        "    status = \"Processing\"\n",
        "    word_labels = os.listdir(\"dataset/\")\n",
        "    wordsresult=[]\n",
        "    for i in range(len(test_chunks)):\n",
        "        pred_ML_RF, pred_ML_KNN, pred_ML_SVM, pred_ML_Vote = speechrecognitiontest(test_chunks[i])\n",
        "        Result = [word_labels[np.int(pred_ML_RF)], word_labels[np.int(pred_ML_KNN)], word_labels[np.int(pred_ML_SVM)],word_labels[np.int(pred_ML_Vote)]]\n",
        "        counter = Counter(Result)\n",
        "        most_occur = counter.most_common(1)\n",
        "        print(most_occur)\n",
        "        wordsresult.append(most_occur)\n",
        "    return wordsresult\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Inputfilename = \"C:\\\\Users\\\\Alina Baber\\\\projects\\\\api\\\\Al-Kawthar\\\\الكَوثَر-001\\\\108001_01.wav\"\n",
        "    result=Main(Inputfilename)\n",
        "    print(result)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGc_dGjW6Glr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}