{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plRKCEEZywtf",
        "outputId": "1be55d34-8875-4dda-9975-04844ca116b4"
      },
      "source": [
        "!pip install spafe\n",
        "!pip install pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.1.2-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.4.1)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.1.2\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot, install\n",
            "Successfully installed install-1.3.4 scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzl-QM6yy1XD",
        "outputId": "0ea6d95c-2729-4ce2-90e4-2c704398f40d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "pJNN4VzmyipJ",
        "outputId": "6d5ddfc9-ecc3-47c7-8264-28360ef77b13"
      },
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "import random\n",
        "import scipy.signal as sg\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import BatchNormalization,Activation,Dropout,LSTM\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "import warnings\n",
        "from seaborn import heatmap, set\n",
        "import scikitplot as skplt\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import norm\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import r2_score, roc_auc_score, roc_curve\n",
        "from scipy import stats  # For in-built method to get PCC\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.svm import SVC\n",
        "#importing libraries\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "import random\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import BatchNormalization,Activation,Dropout,LSTM\n",
        "import copy\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import joblib\n",
        "import scipy.signal as sg\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from spafe.features.lpc import lpc, lpcc\n",
        "#from hmmlearn import hmm\n",
        "from spafe.features.rplp import rplp, plp\n",
        "from sklearn.metrics import classification_report\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "class feature_calculation():\n",
        "    def mfcc_feature(self, audio, sample_rate):\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "\n",
        "        return mfcc # it returns a np.array with size (40,'n') where n is the number of audio frames.\n",
        "\n",
        "    def RMS_feature(self, audio, sample_rate):\n",
        "        rms = librosa.feature.rms(y=audio)\n",
        "\n",
        "        return rms # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def CEN_feature(self, audio, sample_rate):\n",
        "        cen = librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
        "\n",
        "        return cen  # it returns a np.array with size (12,'n') where n is the number of audio frames.\n",
        "\n",
        "    def melspectrogram_feature(self, audio, sample_rate):\n",
        "        melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return melspectrogram # it returns a np.array with size (128,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_centroid_feature(self, audio, sample_rate):\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return spectral_centroid  # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def tonnetz_feature(self, audio, sample_rate):\n",
        "        y = librosa.effects.harmonic(audio)\n",
        "        tonnetz = librosa.feature.tonnetz(y=y, sr=sample_rate)\n",
        "\n",
        "        return tonnetz # it returns a np.array with size (6,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_contrast_feature(self, audio, sample_rate):\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return spectral_contrast # it returns a  np.array with size (7,'n') where n is the number of audio frames.\n",
        "\n",
        "    def poly_feature(self, audio, sample_rate):\n",
        "        poly_features = librosa.feature.poly_features(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return poly_features  # it returns a np.array with size (2,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_rolloff_feature(self, audio, sample_rate):\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.95)\n",
        "\n",
        "        return spectral_rolloff  # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def chroma_stft_feature(self, audio, sample_rate):\n",
        "        chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return chroma_stft  # it returns a np.array with size (12,'n') where n is the number of audio frames.\n",
        "\n",
        "    def zero_crossing_rate_features(self, audio, sample_rate):\n",
        "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
        "\n",
        "        return zero_crossing_rate # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def lpcc_feature(self, audio, sample_rate):\n",
        "        # compute lpccs\n",
        "        lifter = 0\n",
        "        normalize = True\n",
        "        lpccs = lpcc(sig=audio, fs=sample_rate, num_ceps=13, lifter=lifter, normalize=normalize)\n",
        "        return lpccs  # it returns a np.array with size ('n',13) where n is the number of audio frames.\n",
        "\n",
        "    def RPLP_feature(self, audio, sample_rate):\n",
        "        num_ceps = 13\n",
        "        # compute features\n",
        "        rplps = rplp(audio, sample_rate, num_ceps)\n",
        "        return rplps # it returns a np.array with size ('n',13) where n is the number of audio frames.\n",
        "\n",
        "    def pitch_feature(self, audio, sample_rate):\n",
        "        pitches, magnitudes = librosa.core.piptrack(audio, sr=16000, fmin=75, fmax=1600)\n",
        "        return pitches[:200,:]  # it returns a np.array with size (200,'n') where n is the number of audio frames.\n",
        "class DataConversion:\n",
        "  def __init__(self,root=\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy\"):\n",
        "    self.root=root\n",
        "    self.all_wave = []\n",
        "    self.all_label = []\n",
        "    self.all_features=[]\n",
        "    self.MFCC_features=[]\n",
        "    self.MSS_features=[]\n",
        "    self.poly_features=[]\n",
        "    self.ZCR_features=[]\n",
        "\n",
        "    self.MFCCMSS_features=[]\n",
        "    self.MFCCpoly_features=[]\n",
        "    self.MFCCZCR_features=[]\n",
        "\n",
        "    self.MSSpoly_features=[]\n",
        "    self.MSSZCR_features=[]\n",
        "    self.polyZCR_features=[]\n",
        "  def speech_preprocessing(self, sample, sample_rate):\n",
        "    \"\"\"# **Emphasising** preEmphasis, smaller frequency to higher from y(t)=x(t)−αx(t−1\"\"\"\n",
        "    pre_emphasis = 1\n",
        "    emphasized_signal = np.append(sample[0], sample[1:] - pre_emphasis * sample[:-1])\n",
        "    Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "    # Remove silence\n",
        "    y = librosa.effects.split(emphasized_signal, top_db=30)\n",
        "    l = []\n",
        "    for i in y:\n",
        "        l.append(emphasized_signal[i[0]:i[1]])\n",
        "    emphasized_signal = np.concatenate(l, axis=0)\n",
        "    Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "    b, a = sg.butter(4, 1000. / (sample_rate / 2.), 'high')\n",
        "    emphasized_signal_fil = sg.filtfilt(b, a, emphasized_signal)\n",
        "        # fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
        "        # ax.plot(Time, emphasized_signal, lw=1)\n",
        "        # ax.plot(Time, emphasized_signal_fil, lw=1)\n",
        "    return emphasized_signal_fil\n",
        "  def DataToWave(self,root):\n",
        "    sample_rate = 16000\n",
        "    root = root\n",
        "    number_of_sample = 600   #datavalues\n",
        "    labels= os.listdir(root)\n",
        "    features_cal = feature_calculation()\n",
        "    for label in labels:\n",
        "      waves = [f for f in os.listdir(root + '/'+ label) if f.endswith('.wav')]\n",
        "      for wav in waves[:number_of_sample]:\n",
        "        samples, sample_rate = librosa.load(root + '/' + label + '/' + wav, sr = 16000)\n",
        "        samples = librosa.resample(samples, sample_rate, 8000)\n",
        "        #samples= speech_preprocessing( samples, sample_rate)\n",
        "        MFCC = features_cal.mfcc_feature(samples , sample_rate)\n",
        "        #RPLP=features_cal.RPLP_feature(samples , sample_rate)\n",
        "        MSS = features_cal.melspectrogram_feature(samples , sample_rate)\n",
        "        poly = features_cal.poly_feature(samples , sample_rate)\n",
        "        ZCR = features_cal.zero_crossing_rate_features(samples , sample_rate)\n",
        "        samples= speech_preprocessing( samples, sample_rate)\n",
        "            # flatten an array\n",
        "        MFCC = MFCC.flatten()\n",
        "        #RPLP=RPLP.flatten()\n",
        "        MSS = MSS.flatten()\n",
        "        poly = poly.flatten()\n",
        "        ZCR = ZCR.flatten()\n",
        "            # normalizing\n",
        "            # MFCC = normalize(MFCC)\n",
        "        features = np.concatenate(( MFCC,MSS, poly, ZCR))\n",
        "        features1=MFCC\n",
        "        features2=MSS\n",
        "        features3=poly\n",
        "        features4=ZCR\n",
        "\n",
        "        features_combo1 = np.concatenate(( MFCC,MSS))\n",
        "        features_combo2 = np.concatenate(( MFCC poly))\n",
        "        features_combo3 = np.concatenate(( MFCC,ZCR))\n",
        "        features_combo4 = np.concatenate(( MSS, poly))\n",
        "        features_combo5 = np.concatenate(( MSS,ZCR))\n",
        "        features_combo6 = np.concatenate(( poly,ZCR))\n",
        "            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "        features = features[:max_len]\n",
        "                    # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features1.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features1 = np.pad(features1, pad_width=((0, pad_width)), mode='constant')\n",
        "        features1 = features1[:max_len]\n",
        "                            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features2.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features2 = np.pad(features2, pad_width=((0, pad_width)), mode='constant')\n",
        "        features2 = features2[:max_len]\n",
        "                            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features3.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features3 = np.pad(features3, pad_width=((0, pad_width)), mode='constant')\n",
        "        features3 = features3[:max_len]\n",
        "                            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features4.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features4 = np.pad(features4, pad_width=((0, pad_width)), mode='constant')\n",
        "        features4 = features4[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo1.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo1 = np.pad(features_combo1, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo1= features_combo1[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo2.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo2 = np.pad(features_combo2, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo2 = features_combo2[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo3.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo3 = np.pad(features_combo3, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo3 = features_combo3[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo4.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo4 = np.pad(features_combo4, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo4 = features_combo4[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo5.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo5= np.pad(features_combo5, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo5 = features_combo5[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo6.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo6 = np.pad(features_combo6, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo6 = features_combo6[:max_len]\n",
        "\n",
        "        if(len(samples)== 8000):\n",
        "                self.all_features.append(features)\n",
        "                self.MFCC_features.append(features1)\n",
        "                self.MSS_features.append(features2)\n",
        "                self.poly_features.append(features3)\n",
        "                self.ZCR_features.append(features4)\n",
        "                self.MFCCMSS_features.append(features_combo1)\n",
        "                self.MFCCpoly_features.append(features_combo2)\n",
        "                self.MFCCZCR_features.append(features_combo3)\n",
        "                self.MSSpoly_features.appendfeatures_combo4)\n",
        "                self.MSSZCR_features.append(features_combo5)\n",
        "                self.polyZCR_features.append(features_combo6)\n",
        "                self.all_wave.append(samples)\n",
        "                self.all_label.append(label)\n",
        "    #printing all labels\n",
        "    le = LabelEncoder()\n",
        "    y=le.fit_transform(all_label)\n",
        "    classes= list(le.classes_)\n",
        "\n",
        "    #DP=DataPrepration(all_label)\n",
        "    #DP.label_encoder()\n",
        "\n",
        "    print(classes)\n",
        "    y=np_utils.to_categorical(y, num_classes=len(labels))\n",
        "    all_wave = np.array(all_wave).reshape(-1,8000,1)\n",
        "    all_features = np.array(all_features).reshape(-1,8000,1)\n",
        "    MFCC_features = np.array(MFCC_features).reshape(-1,8000,1)\n",
        "    MSS_features = np.array(MSS_features).reshape(-1,8000,1)\n",
        "    poly_features = np.array(poly_features).reshape(-1,8000,1)\n",
        "    ZCR_features = np.array(ZCR_features).reshape(-1,8000,1)\n",
        "\n",
        "    MFCCMSS_features = np.array(MFCCMSS_features).reshape(-1,8000,1)\n",
        "    MFCCpoly_features = np.array(MFCCpoly_features).reshape(-1,8000,1)\n",
        "    MFCCZCR_features = np.array(MFCCZCR_features).reshape(-1,8000,1)\n",
        "    MSSpoly_features = np.array(MSSpoly_features).reshape(-1,8000,1)\n",
        "    MSSZCR_features = np.array(MSSZCR_features).reshape(-1,8000,1)\n",
        "    polyZCR_features = np.array(polyZCR_features).reshape(-1,8000,1)\n",
        "\n",
        "    x_tr_sample, x_val_sample, y_tr_sample, y_val_sample = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_all_features, x_val_all_features, y_tr_all_features, y_val_all_features = train_test_split(np.array(all_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_MFCC_features, x_val_MFCC_features, y_tr_MFCC_features, y_val_MFCC_features = train_test_split(np.array(MFCC_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_MSS_features, x_val_MSS_features, y_tr_MSS_features, y_val_MSS_features = train_test_split(np.array(MSS_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_poly_features, x_val_poly_features, y_tr_poly_features, y_val_poly_features = train_test_split(np.array(poly_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_ZCR_features, x_val_ZCR_features, y_tr_ZCR_features, y_val_ZCR_features = train_test_split(np.array(ZCR_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "\n",
        "    x_tr_MFCCMSS_features, x_val_MFCCMSS_features, y_tr_MFCCMSS_features, y_val_MFCCMSS_features = train_test_split(np.array(MFCCMSS_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_MFCCpoly_features, x_val_MFCCpoly_features, y_tr_ZCR_MFCCpoly_features, y_val_MFCCpoly_features = train_test_split(np.array(MFCCpoly_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_MFCCZCR_features, x_val_MFCCZCR_features, y_tr_MFCCZCR_features, y_val_MFCCZCR_features = train_test_split(np.array(MFCCZCR_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_MSSpoly_features, x_val_MSSpoly_features, y_tr_MSSpoly_features, y_val_MSSpoly_features = train_test_split(np.array(MSSpoly_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_MSSZCR_featuress, x_val_MSSZCR_features, y_tr_MSSZCR_features, y_val_MSSZCR_featuress = train_test_split(np.array(MSSZCR_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_tr_polyZCR_features, x_val_polyZCR_features, y_tr_polyZCR_features, y_val_polyZCR_features = train_test_split(np.array(polyZCR_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "\n",
        "    wave_train= [x_tr_sample, y_tr_sample]\n",
        "    wave_test= [x_val_sample, y_val_sample]\n",
        "\n",
        "    allfeatures_train=[ x_tr_all_features,  y_tr_all_features]\n",
        "    allfeatures_test= [ x_val_all_features, y_val_all_features]\n",
        "\n",
        "    mfcc_train=[x_tr_MFCC_features, y_tr_MFCC_features]\n",
        "    mfcc_tests=[x_val_MFCC_features ,y_val_MFCC_features]\n",
        "    mss_train=[x_tr_MSS_features, y_tr_MSS_features]\n",
        "    mss_train=[x_val_MSS_features y_val_MSS_features]\n",
        "    poly_train=[x_tr_poly_features, y_tr_poly_features]\n",
        "    poly_train=[x_val_poly_features, y_val_poly_features]\n",
        "    zcr_train= [x_tr_ZCR_features, y_tr_ZCR_features]\n",
        "    zcr_train= [x_val_ZCR_features,y_val_ZCR_features]\n",
        "\n",
        "    mfccmss_train=[x_tr_MFCCMSS_features, y_tr_MFCCMSS_features]\n",
        "    mfccmss_test=[x_val_MFCCMSS_features, y_val_MFCCMSS_features]\n",
        "    mfccpoly_train=[x_tr_MFCCMSS_features, y_tr_MFCCMSS_features]\n",
        "    mfccpoly_test=[x_val_MFCCMSS_features, y_val_MFCCMSS_features]\n",
        "    mfcczcr_train=[ x_tr_MFCCZCR_features, y_tr_MFCCZCR_features]\n",
        "    mfcczcr_test=[ x_val_MFCCZCR_features, y_val_MFCCZCR_features]\n",
        "    msspoly_train=[x_tr_MSSpoly_features,y_tr_MSSpoly_features]\n",
        "    msspoly_test=[x_tr_MSSpoly_features, x_val_MSSpoly_features, y_tr_MSSpoly_features, y_val_MSSpoly_features]\n",
        "    msszcr_train=[x_tr_MSSZCR_featuress, x_val_MSSZCR_features, y_tr_MSSZCR_features, y_val_MSSZCR_featuress]\n",
        "    polyzcr_train=[x_tr_polyZCR_features, x_val_polyZCR_features, y_tr_polyZCR_features, y_val_polyZCR_features]\n",
        "    return\n",
        "\n",
        "class Models:\n",
        "  def cnn_model(self):\n",
        "    inputs = Input(shape=(6000,1))\n",
        "    #First Conv1D layer\n",
        "    conv = Conv1D(8,13, padding='same', activation='relu', strides=1)(inputs)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Second Conv1D layer\n",
        "    conv = Conv1D(16, 11, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Third Conv1D layer\n",
        "    conv = Conv1D(32, 9, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Fourth Conv1D layer\n",
        "    conv = Conv1D(64, 7, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Fourth Conv1D layer\n",
        "    conv = Conv1D(128, 5, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "\n",
        "    #Flatten layer\n",
        "    conv = Flatten()(conv)\n",
        "\n",
        "    #Dense Layer 1\n",
        "    conv = Dense(256, activation='relu')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Dense Layer 2\n",
        "    conv = Dense(128, activation='relu')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "\n",
        "    classification_output = keras.layers.Dense(len(labels), activation=\"softmax\")(conv)\n",
        "\n",
        "    self.model = Model(inputs, outputs=classification_output)#outputs)\n",
        "    return self.model , self.model.summary()\n",
        "\n",
        "  def cnn_compile(self):\n",
        "    return self.model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "  def cnn_fit(self,x_tr,y_tr):\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20, min_delta=0.0001)\n",
        "    mc = ModelCheckpoint('/content/drive/MyDrive/Models/best_modelcnnFEverynoisy.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    return self.model.fit(x_tr, y_tr ,epochs=1000, batch_size=200,callbacks=[es,mc], validation_data=(x_val,y_val))\n",
        "  def model_save(self,path):\n",
        "    return self.model.save(path)\n",
        "\n",
        "  def results(self, target_test, predicted_test, ModelName, labels):\n",
        "    target_names = labels\n",
        "    print(classification_report(target_test, predicted_test, target_names=target_names))\n",
        "    y_test = target_test\n",
        "    preds = predicted_test\n",
        "    rms = np.sqrt(np.mean(np.power((np.array(y_test) - np.array(preds)), 2)))\n",
        "    score = r2_score(y_test, preds)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    mse = mean_squared_error(y_test, preds)\n",
        "    pearson_coef, p_value = stats.pearsonr(y_test, preds)\n",
        "\n",
        "    print(\"root mean square:\", rms)\n",
        "    print(\"score:\", score)\n",
        "    print(\"mean absolute error:\", mae)\n",
        "    print(\"mean squared error:\", mse)\n",
        "    print(\"pearson_coef:\", pearson_coef)\n",
        "    print(\"p_value:\", p_value)\n",
        "    print(\"=======================================================================\\n\\n\")\n",
        "    skplt.metrics.plot_confusion_matrix(\n",
        "        y_test,\n",
        "        preds,\n",
        "        figsize=(10, 6), title=\"Confusion matrix\\n Deposite Category \" + ModelName)\n",
        "    plt.xlim(-0.5, len(np.unique(y_test)) - 0.5)\n",
        "    plt.ylim(len(np.unique(y_test)) - 0.5, -0.5)\n",
        "    plt.savefig('cvroc.png')\n",
        "    plt.show()\n",
        "    # Bagging\n",
        "    ns_probs = [0 for _ in range(len(y_test))]\n",
        "    # lr_probs = predictions\n",
        "    # best_found_fina=individual_list\n",
        "    ns_aucb = roc_auc_score(y_test, ns_probs)\n",
        "    lr_aucb = roc_auc_score(y_test, preds)\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
        "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "    plt.plot(precision, recall, thresholds, marker='.', label='Logistic')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "    # summarize scores\n",
        "    print('No Skill: ROC AUC=%.3f' % (ns_aucb))\n",
        "    print('Logistic: ROC AUC=%.3f' % (lr_aucb))\n",
        "    # calculate roc curves\n",
        "    ns_fprb, ns_tprb, _ = roc_curve(y_test, ns_probs)\n",
        "    lr_fprb, lr_tprb, _ = roc_curve(y_test, preds)\n",
        "    # plot the roc curve for the model\n",
        "    plt.plot(ns_fprb, ns_tprb, linestyle='--', label='No Skill')\n",
        "    plt.plot(lr_fprb, lr_tprb, marker='.', label='target')\n",
        "    plt.plot(lr_fprb, lr_tprb, marker='.', label=ModelName)\n",
        "    # axis labels\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    plt.title(\"ROC Curve Graph\")\n",
        "    plt.savefig('comparisonroc.png')\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Model_test:\n",
        "  def cnn_predict(self,audio):\n",
        "    prob=self.model.predict(audio.reshape(1,8000,1))\n",
        "    index=np.argmax(prob[0])\n",
        "    return classes[index]\n",
        "  def evaluation(self,x_test,y_test,batch_size=48):\n",
        "    results = self.model.evaluate(x_test, y_test, batch_size)\n",
        "    return results\n",
        "\n",
        "class Model_accuracy:\n",
        "  def accuracy():\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='test')\n",
        "    plt.legend()\n",
        "    return plt.show()\n",
        "\n",
        "class DataPrepration:\n",
        "  def __init__(self,all_label):\n",
        "    self.all_label = all_label\n",
        "  def labels_encoder(self):\n",
        "    le = LabelEncoder()\n",
        "    y=le.fit_transform(self.all_label)\n",
        "    classes= list(le.classes_)\n",
        "    return classes\n",
        "  def spliting_data(self,test_size,random_state,shuffle):\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),\n",
        "                                                test_size,\n",
        "                                                np.array(y),\n",
        "                                                random_state,\n",
        "                                                shuffle,stratify=y)\n",
        "    return x_tr,x_val,y_tr,y_val\n",
        "\n",
        "\n",
        "\n",
        "sample_rate = 16000\n",
        "root = \"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy\"\n",
        "number_of_sample = 600   #datavalues\n",
        "\n",
        "labels= os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy\")\n",
        "features_cal = feature_calculation()\n",
        "\n",
        "MFCC_features=[]\n",
        "MSS_features=[]\n",
        "poly_features=[]\n",
        "ZCR_features=[]\n",
        "\n",
        "MFCCMSS_features=[]\n",
        "MFCCpoly_features=[]\n",
        "MFCCZCR_features=[]\n",
        "\n",
        "MSSpoly_features=[]\n",
        "MSSZCR_features=[]\n",
        "\n",
        "polyZCR_features=[]\n",
        "\n",
        "poly_features=[]\n",
        "ZCR_features=[]\n",
        "all_features=[]\n",
        "all_wave = []\n",
        "all_label = []\n",
        "\n",
        "for label in labels:\n",
        "    waves = [f for f in os.listdir(root + '/'+ label) if f.endswith('.wav')]\n",
        "    for wav in waves[:number_of_sample]:\n",
        "        samples, sample_rate = librosa.load(root + '/' + label + '/' + wav, sr = 16000)\n",
        "        samples = librosa.resample(samples, sample_rate, 8000)\n",
        "        #samples= speech_preprocessing( samples, sample_rate)\n",
        "        MFCC = features_cal.mfcc_feature(samples , sample_rate)\n",
        "        #RPLP=features_cal.RPLP_feature(samples , sample_rate)\n",
        "        MSS = features_cal.melspectrogram_feature(samples , sample_rate)\n",
        "        poly = features_cal.poly_feature(samples , sample_rate)\n",
        "        ZCR = features_cal.zero_crossing_rate_features(samples , sample_rate)\n",
        "        samples= speech_preprocessing( samples, sample_rate)\n",
        "            # flatten an array\n",
        "        MFCC = MFCC.flatten()\n",
        "        #RPLP=RPLP.flatten()\n",
        "        MSS = MSS.flatten()\n",
        "        poly = poly.flatten()\n",
        "        ZCR = ZCR.flatten()\n",
        "            # normalizing\n",
        "            # MFCC = normalize(MFCC)\n",
        "        features = np.concatenate(( MFCC,MSS, poly, ZCR))\n",
        "        features1=MFCC\n",
        "        features2=MSS\n",
        "        features3=poly\n",
        "        features4=ZCR\n",
        "\n",
        "        features_combo1 = np.concatenate(( MFCC,MSS))\n",
        "        features_combo2 = np.concatenate(( MFCC poly))\n",
        "        features_combo3 = np.concatenate(( MFCC,ZCR))\n",
        "        features_combo4 = np.concatenate(( MSS, poly))\n",
        "        features_combo5 = np.concatenate(( MSS,ZCR))\n",
        "        features_combo6 = np.concatenate(( poly,ZCR))\n",
        "            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "        features = features[:max_len]\n",
        "                    # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features1.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features1 = np.pad(features1, pad_width=((0, pad_width)), mode='constant')\n",
        "        features1 = features1[:max_len]\n",
        "                            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features2.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features2 = np.pad(features2, pad_width=((0, pad_width)), mode='constant')\n",
        "        features2 = features2[:max_len]\n",
        "                            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features3.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features3 = np.pad(features3, pad_width=((0, pad_width)), mode='constant')\n",
        "        features3 = features3[:max_len]\n",
        "                            # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features4.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features4 = np.pad(features4, pad_width=((0, pad_width)), mode='constant')\n",
        "        features4 = features4[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo1.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo1 = np.pad(features_combo1, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo1= features_combo1[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo2.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo2 = np.pad(features_combo2, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo2 = features_combo2[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo3.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo3 = np.pad(features_combo3, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo3 = features_combo3[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo4.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo4 = np.pad(features_combo4, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo4 = features_combo4[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo5.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo5= np.pad(features_combo5, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo5 = features_combo5[:max_len]\n",
        "         # padding and trimming\n",
        "        max_len = 8000\n",
        "        pad_width = max_len - features_combo6.shape[0]\n",
        "        if pad_width > 0:\n",
        "           features_combo6 = np.pad(features_combo6, pad_width=((0, pad_width)), mode='constant')\n",
        "        features_combo6 = features_combo6[:max_len]\n",
        "\n",
        "        if(len(samples)== 8000):\n",
        "            all_features.append(features)\n",
        "            MFCC_features.append(features1)\n",
        "            MSS_features.append(features2)\n",
        "            poly_features.append(features3)\n",
        "            ZCR_features.append(features4)\n",
        "            MFCCMSS_features.append(features_combo1)\n",
        "            MFCCpoly_features.append(features_combo2)\n",
        "            MFCCZCR_features.append(features_combo3)\n",
        "            MSSpoly_features.appendfeatures_combo4)\n",
        "            MSSZCR_features.append(features_combo5)\n",
        "            polyZCR_features.append(features_combo6)\n",
        "            all_wave.append(samples)\n",
        "            all_label.append(label)\n",
        "#printing all labels\n",
        "le = LabelEncoder()\n",
        "y=le.fit_transform(all_label)\n",
        "classes= list(le.classes_)\n",
        "\n",
        "#DP=DataPrepration(all_label)\n",
        "#DP.label_encoder()\n",
        "\n",
        "print(classes)\n",
        "y=np_utils.to_categorical(y, num_classes=len(labels))\n",
        "all_wave = np.array(all_wave).reshape(-1,8000,1)\n",
        "all_features = np.array(all_features).reshape(-1,8000,1)\n",
        "MFCC_features = np.array(MFCC_features).reshape(-1,8000,1)\n",
        "MSS_features = np.array(MSS_features).reshape(-1,8000,1)\n",
        "poly_features = np.array(poly_features).reshape(-1,8000,1)\n",
        "ZCR_features = np.array(ZCR_features).reshape(-1,8000,1)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(MFCC_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(MSS_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(poly_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(ZCR_features),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "\n",
        "ML_model_implimentation(self,feature_train,target_train,feature_test,target_test,labels,featurecombo)\n",
        "model=Models()\n",
        "model.cnn_model()\n",
        "model.cnn_compile()\n",
        "history=model.cnn_fit(x_tr, y_tr)\n",
        "\n",
        "#Accuracy Graph\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "model.model_save('/content/drive/MyDrive/Models/model_DlcnnFEverynoisy.h5')\n",
        "from tensorflow.keras.models import load_model\n",
        "loded=load_model('/content/drive/MyDrive/Models/model_DlcnnFEverynoisy.h5')\n",
        "test=loded.evaluate(x_val,y_val)\n",
        "print(\"Test Accuracy: \" , (test[1]*100),'%')\n",
        "train=loded.evaluate(x_tr,y_tr)\n",
        "print(\"Test Accuracy: \" , (train[1]*100),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3d7b35eccc66>\"\u001b[0;36m, line \u001b[0;32m231\u001b[0m\n\u001b[0;31m    features_combo2 = np.concatenate(( MFCC poly))\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CQrOi7EE7Cd"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "loded=load_model('/content/drive/MyDrive/Models/model_DlcnnFEverynoisy.h5')\n",
        "test=loded.evaluate(x_val,y_val)\n",
        "print(\"Test Accuracy: \" , (test[1]*100),'%')\n",
        "train=loded.evaluate(x_tr,y_tr)\n",
        "print(\"Test Accuracy: \" , (train[1]*100),'%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}