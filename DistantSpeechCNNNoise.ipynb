{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plRKCEEZywtf",
        "outputId": "777a0dee-cc69-4793-8f1a-eaac3bc7b493"
      },
      "source": [
        "\n",
        "!pip install spafe\n",
        "!pip install pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.1.2-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.19.5)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.1.2\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot, install\n",
            "Successfully installed install-1.3.4 scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzl-QM6yy1XD",
        "outputId": "8d77ee33-cc0d-41a5-ff1f-1f09804bfd98"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pJNN4VzmyipJ",
        "outputId": "0f066260-6c0d-4b8d-f310-a71eba5e7a32"
      },
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "import random\n",
        "import scipy.signal as sg\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import BatchNormalization,Activation,Dropout,LSTM\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "import warnings\n",
        "from seaborn import heatmap, set\n",
        "import scikitplot as skplt\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import norm\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import r2_score, roc_auc_score, roc_curve\n",
        "from scipy import stats  # For in-built method to get PCC\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.svm import SVC\n",
        "#importing libraries\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "import random\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import BatchNormalization,Activation,Dropout,LSTM\n",
        "import copy\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import joblib\n",
        "import scipy.signal as sg\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from spafe.features.lpc import lpc, lpcc\n",
        "#from hmmlearn import hmm\n",
        "from spafe.features.rplp import rplp, plp\n",
        "from sklearn.metrics import classification_report\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "class feature_calculation():\n",
        "    def mfcc_feature(self, audio, sample_rate):\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "\n",
        "        return mfcc # it returns a np.array with size (40,'n') where n is the number of audio frames.\n",
        "\n",
        "    def RMS_feature(self, audio, sample_rate):\n",
        "        rms = librosa.feature.rms(y=audio)\n",
        "\n",
        "        return rms # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def CEN_feature(self, audio, sample_rate):\n",
        "        cen = librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
        "\n",
        "        return cen  # it returns a np.array with size (12,'n') where n is the number of audio frames.\n",
        "\n",
        "    def melspectrogram_feature(self, audio, sample_rate):\n",
        "        melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return melspectrogram # it returns a np.array with size (128,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_centroid_feature(self, audio, sample_rate):\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return spectral_centroid  # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def tonnetz_feature(self, audio, sample_rate):\n",
        "        y = librosa.effects.harmonic(audio)\n",
        "        tonnetz = librosa.feature.tonnetz(y=y, sr=sample_rate)\n",
        "\n",
        "        return tonnetz # it returns a np.array with size (6,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_contrast_feature(self, audio, sample_rate):\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return spectral_contrast # it returns a  np.array with size (7,'n') where n is the number of audio frames.\n",
        "\n",
        "    def poly_feature(self, audio, sample_rate):\n",
        "        poly_features = librosa.feature.poly_features(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return poly_features  # it returns a np.array with size (2,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_rolloff_feature(self, audio, sample_rate):\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.95)\n",
        "\n",
        "        return spectral_rolloff  # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def chroma_stft_feature(self, audio, sample_rate):\n",
        "        chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return chroma_stft  # it returns a np.array with size (12,'n') where n is the number of audio frames.\n",
        "\n",
        "    def zero_crossing_rate_features(self, audio, sample_rate):\n",
        "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
        "\n",
        "        return zero_crossing_rate # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def lpcc_feature(self, audio, sample_rate):\n",
        "        # compute lpccs\n",
        "        lifter = 0\n",
        "        normalize = True\n",
        "        lpccs = lpcc(sig=audio, fs=sample_rate, num_ceps=13, lifter=lifter, normalize=normalize)\n",
        "        return lpccs  # it returns a np.array with size ('n',13) where n is the number of audio frames.\n",
        "\n",
        "    def RPLP_feature(self, audio, sample_rate):\n",
        "        num_ceps = 13\n",
        "        # compute features\n",
        "        rplps = rplp(audio, sample_rate, num_ceps)\n",
        "        return rplps # it returns a np.array with size ('n',13) where n is the number of audio frames.\n",
        "\n",
        "    def pitch_feature(self, audio, sample_rate):\n",
        "        pitches, magnitudes = librosa.core.piptrack(audio, sr=16000, fmin=75, fmax=1600)\n",
        "        return pitches[:200,:]  # it returns a np.array with size (200,'n') where n is the number of audio frames.\n",
        "class DataConversion:\n",
        "  def __init__(self,root=\"/content/drive/MyDrive/archive (1)/augmented_dataset\"):\n",
        "    self.root=root\n",
        "    self.all_wave = []\n",
        "    self.all_label = []\n",
        "    self.all_features=[]\n",
        "  def speech_preprocessing(self, sample, sample_rate):\n",
        "    \"\"\"# **Emphasising** preEmphasis, smaller frequency to higher from y(t)=x(t)−αx(t−1\"\"\"\n",
        "    pre_emphasis = 1\n",
        "    emphasized_signal = np.append(sample[0], sample[1:] - pre_emphasis * sample[:-1])\n",
        "    Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "    # Remove silence\n",
        "    y = librosa.effects.split(emphasized_signal, top_db=30)\n",
        "    l = []\n",
        "    for i in y:\n",
        "        l.append(emphasized_signal[i[0]:i[1]])\n",
        "    emphasized_signal = np.concatenate(l, axis=0)\n",
        "    Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "    b, a = sg.butter(4, 1000. / (sample_rate / 2.), 'high')\n",
        "    emphasized_signal_fil = sg.filtfilt(b, a, emphasized_signal)\n",
        "        # fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
        "        # ax.plot(Time, emphasized_signal, lw=1)\n",
        "        # ax.plot(Time, emphasized_signal_fil, lw=1)\n",
        "    return emphasized_signal_fil\n",
        "  def DataToWave(self,number_of_samples):\n",
        "    features_cal = feature_calculation()\n",
        "    for label in labels:\n",
        "        waves = [f for f in os.listdir(self.root + '/'+ label) if f.endswith('.wav')]\n",
        "        for wav in waves[:number_of_sample]:\n",
        "            samples, sample_rate = librosa.load(self.root + '/' + label + '/' + wav, sr = 16000)\n",
        "            samples = librosa.resample(samples, sample_rate, 8000)\n",
        "            samples = self.speech_preprocessing( samples, sample_rate)\n",
        "            MFCC = features_cal.mfcc_feature(samples , sample_rate)\n",
        "            RPLP=features_cal.RPLP_feature(samples , sample_rate)\n",
        "            MSS = features_cal.melspectrogram_feature(samples , sample_rate)\n",
        "            poly = features_cal.poly_feature(samples , sample_rate)\n",
        "            ZCR = features_cal.zero_crossing_rate_features(samples , sample_rate)\n",
        "            # flatten an array\n",
        "            MFCC = MFCC.flatten()\n",
        "            RPLP=RPLP.flatten()\n",
        "            MSS = MSS.flatten()\n",
        "            poly = poly.flatten()\n",
        "            ZCR = ZCR.flatten()\n",
        "            # normalizing\n",
        "            # MFCC = normalize(MFCC)\n",
        "            features = np.concatenate(( MFCC,RPLP ,MSS, poly, ZCR))\n",
        "            # padding and trimming\n",
        "            max_len = 6000\n",
        "            pad_width = max_len - features.shape[0]\n",
        "            if pad_width > 0:\n",
        "              features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "            features = features[:max_len]\n",
        "            if(len(samples)== 8000):\n",
        "                all_features.append(features)\n",
        "                all_wave.append(samples)\n",
        "                all_label.append(label)\n",
        "    return self.all_features,self.all_wave,self.all_label\n",
        "\n",
        "class Models:\n",
        "  def cnn_model(self):\n",
        "    inputs = Input(shape=(8000,1))\n",
        "    #First Conv1D layer\n",
        "    conv = Conv1D(8,13, padding='same', activation='relu', strides=1)(inputs)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Second Conv1D layer\n",
        "    conv = Conv1D(16, 11, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Third Conv1D layer\n",
        "    conv = Conv1D(32, 9, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Fourth Conv1D layer\n",
        "    conv = Conv1D(64, 7, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Fourth Conv1D layer\n",
        "    conv = Conv1D(128, 5, padding='same', activation='relu', strides=1)(conv)\n",
        "    conv = MaxPooling1D(3)(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "\n",
        "    #Flatten layer\n",
        "    conv = Flatten()(conv)\n",
        "\n",
        "    #Dense Layer 1\n",
        "    conv = Dense(256, activation='relu')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    #Dense Layer 2\n",
        "    conv = Dense(128, activation='relu')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "\n",
        "    classification_output = keras.layers.Dense(len(labels), activation=\"softmax\")(conv)\n",
        "\n",
        "    self.model = Model(inputs, outputs=classification_output)#outputs)\n",
        "    return self.model , self.model.summary()\n",
        "\n",
        "  def cnn_compile(self):\n",
        "    return self.model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "  def cnn_fit(self,x_tr,y_tr):\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20, min_delta=0.0001)\n",
        "    mc = ModelCheckpoint('/content/drive/MyDrive/Models/best_modelcnnFEnoisy.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    return self.model.fit(x_tr, y_tr ,epochs=1000, batch_size=200,callbacks=[es,mc], validation_data=(x_val,y_val))\n",
        "  def model_save(self,path):\n",
        "    return self.model.save(path)\n",
        "\n",
        "class Model_test:\n",
        "  def cnn_predict(self,audio):\n",
        "    prob=self.model.predict(audio.reshape(1,8000,1))\n",
        "    index=np.argmax(prob[0])\n",
        "    return classes[index]\n",
        "  def evaluation(self,x_test,y_test,batch_size=48):\n",
        "    results = self.model.evaluate(x_test, y_test, batch_size)\n",
        "    return results\n",
        "\n",
        "class Model_accuracy:\n",
        "  def accuracy():\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='test')\n",
        "    plt.legend()\n",
        "    return plt.show()\n",
        "\n",
        "class DataPrepration:\n",
        "  def __init__(self,all_label):\n",
        "    self.all_label = all_label\n",
        "  def labels_encoder(self):\n",
        "    le = LabelEncoder()\n",
        "    y=le.fit_transform(self.all_label)\n",
        "    classes= list(le.classes_)\n",
        "    return classes\n",
        "  def spliting_data(self,test_size,random_state,shuffle):\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),\n",
        "                                                test_size,\n",
        "                                                np.array(y),\n",
        "                                                random_state,\n",
        "                                                shuffle,stratify=y)\n",
        "    return x_tr,x_val,y_tr,y_val\n",
        "\n",
        "#main\n",
        "def speech_preprocessing(sample, sample_rate):\n",
        "    \"\"\"# **Emphasising** preEmphasis, smaller frequency to higher from y(t)=x(t)−αx(t−1\"\"\"\n",
        "    pre_emphasis = 1\n",
        "    emphasized_signal = np.append(sample[0], sample[1:] - pre_emphasis * sample[:-1])\n",
        "    Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "    # Remove silence\n",
        "    y = librosa.effects.split(emphasized_signal, top_db=30)\n",
        "    l = []\n",
        "    for i in y:\n",
        "        l.append(emphasized_signal[i[0]:i[1]])\n",
        "    emphasized_signal = np.concatenate(l, axis=0)\n",
        "    Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "    b, a = sg.butter(4, 1000. / (sample_rate / 2.), 'high')\n",
        "    emphasized_signal_fil = sg.filtfilt(b, a, emphasized_signal)\n",
        "        # fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
        "        # ax.plot(Time, emphasized_signal, lw=1)\n",
        "        # ax.plot(Time, emphasized_signal_fil, lw=1)\n",
        "    return emphasized_signal_fil\n",
        "sample_rate = 16000\n",
        "root = \"/content/drive/MyDrive/archive (1)/augmented_dataset\"\n",
        "number_of_sample = 600   #datavalues\n",
        "\n",
        "labels= os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset\")\n",
        "features_cal = feature_calculation()\n",
        "all_features=[]\n",
        "all_wave = []\n",
        "all_label = []\n",
        "\n",
        "for label in labels:\n",
        "    waves = [f for f in os.listdir(root + '/'+ label) if f.endswith('.wav')]\n",
        "    for wav in waves[:number_of_sample]:\n",
        "        samples, sample_rate = librosa.load(root + '/' + label + '/' + wav, sr = 16000)\n",
        "        samples = librosa.resample(samples, sample_rate, 8000)\n",
        "        #samples= speech_preprocessing( samples, sample_rate)\n",
        "        #MFCC = features_cal.mfcc_feature(samples , sample_rate)\n",
        "        #RPLP=features_cal.RPLP_feature(samples , sample_rate)\n",
        "        #MSS = features_cal.melspectrogram_feature(samples , sample_rate)\n",
        "        #poly = features_cal.poly_feature(samples , sample_rate)\n",
        "        #ZCR = features_cal.zero_crossing_rate_features(samples , sample_rate)\n",
        "        #samples= speech_preprocessing( samples, sample_rate)\n",
        "            # flatten an array\n",
        "        #MFCC = MFCC.flatten()\n",
        "        #RPLP=RPLP.flatten()\n",
        "        #MSS = MSS.flatten()\n",
        "        #poly = poly.flatten()\n",
        "        #ZCR = ZCR.flatten()\n",
        "            # normalizing\n",
        "            # MFCC = normalize(MFCC)\n",
        "        #features = np.concatenate(( MFCC,RPLP ,MSS, poly, ZCR))\n",
        "            # padding and trimming\n",
        "        max_len = 6000\n",
        "        #pad_width = max_len - features.shape[0]\n",
        "        #if pad_width > 0:\n",
        "        #   features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "        #features = features[:max_len]\n",
        "        if(len(samples)== 8000):\n",
        "            #all_features.append(features)\n",
        "            all_wave.append(samples)\n",
        "            all_label.append(label)\n",
        "#printing all labels\n",
        "le = LabelEncoder()\n",
        "y=le.fit_transform(all_label)\n",
        "classes= list(le.classes_)\n",
        "\n",
        "#DP=DataPrepration(all_label)\n",
        "#DP.label_encoder()\n",
        "\n",
        "print(classes)\n",
        "y=np_utils.to_categorical(y, num_classes=len(labels))\n",
        "all_wave = np.array(all_wave).reshape(-1,8000,1)\n",
        "#all_features = np.array(all_features).reshape(-1,8000,1)\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "\n",
        "#model=Models()\n",
        "#model.cnn_model()\n",
        "#model.cnn_compile()\n",
        "#history=model.cnn_fit(x_tr, y_tr)\n",
        "\n",
        "#Accuracy Graph\n",
        "#plt.plot(history.history['loss'], label='train')\n",
        "#plt.plot(history.history['val_loss'], label='test')\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "def results(target_test, predicted_test,ModelName,labels):\n",
        "  target_names = labels\n",
        "  print(classification_report(target_test, predicted_test, target_names=target_names))\n",
        "  y_test = target_test\n",
        "  preds = predicted_test\n",
        "  rms = np.sqrt(np.mean(np.power((np.array(y_test) - np.array(preds)), 2)))\n",
        "  score = r2_score(y_test, preds)\n",
        "  mae = mean_absolute_error(y_test, preds)\n",
        "  mse = mean_squared_error(y_test, preds)\n",
        "  pearson_coef, p_value = stats.pearsonr(y_test, preds)\n",
        "\n",
        "  print(\"root mean square:\", rms)\n",
        "  print(\"score:\", score)\n",
        "  print(\"mean absolute error:\", mae)\n",
        "  print(\"mean squared error:\", mse)\n",
        "  print(\"pearson_coef:\", pearson_coef)\n",
        "  print(\"p_value:\", p_value)\n",
        "  print(\"=======================================================================\\n\\n\")\n",
        "  skplt.metrics.plot_confusion_matrix(\n",
        "  y_test,\n",
        "  preds,\n",
        "  figsize=(10, 6), title=\"Confusion matrix\\n Deposite Category \"+ModelName)\n",
        "  plt.xlim(-0.5, len(np.unique(y_test)) - 0.5)\n",
        "  plt.ylim(len(np.unique(y_test)) - 0.5, -0.5)\n",
        "  plt.savefig('cvroc.png')\n",
        "  plt.show()\n",
        "model.model_save('/content/drive/MyDrive/Models/model_DLccnFEnonoisy.h5')\n",
        "from tensorflow.keras.models import load_model\n",
        "loded=load_model('/content/drive/MyDrive/Models/model_DLccnFEnonoisy.h5')\n",
        "test=loded.evaluate(x_val,y_val)\n",
        "\n",
        "print(\"Test Accuracy: \" , (test[1]*100),'%')\n",
        "train=loded.evaluate(x_tr,y_tr)\n",
        "predicted_test=loded.predict(x_tr)\n",
        "results(target_test, predicted_test,\"CNN\",labels)\n",
        "results(target_test, predicted_test,\"LSTM\",labels)\n",
        "print(\"Test Accuracy: \" , (train[1]*100),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a2cdca05f831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mwaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwav\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwaves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumber_of_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m#samples= speech_preprocessing( samples, sample_rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfilesystemencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_open_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "4CQrOi7EE7Cd",
        "outputId": "6f837401-8f5b-42fd-d1a8-8dfdc16fe5c1"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "loded=load_model('/content/drive/MyDrive/Models/model_DLcnnFEnonoisy.h5')\n",
        "test=loded.evaluate(x_val,y_val)\n",
        "print(\"Test Accuracy: \" , (test[1]*100),'%')\n",
        "train=loded.evaluate(x_tr,y_tr)\n",
        "print(\"Test Accuracy: \" , (train[1]*100),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7df326d9876a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Models/model_DLcnnFEnonoisy.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy: \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 121\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/drive/MyDrive/Models/model_DLcnnFEnonoisy.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    }
  ]
}