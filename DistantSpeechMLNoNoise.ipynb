{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC0DO8bJ5m8p",
        "outputId": "2e309085-329e-4921-f4b8-9e2123284d5b"
      },
      "source": [
        "!pip install spafe\n",
        "!pip install pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.1.2-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.4.1)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.1.2\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot, install\n",
            "Successfully installed install-1.3.4 scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2qsz9QG5oBY",
        "outputId": "418ed80e-4ba7-4f97-8de4-6133d70231bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzvv43eq5A7_",
        "outputId": "eb1c28b5-7c09-4c0d-f86a-21e07d9a6bef"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "import warnings\n",
        "from seaborn import heatmap, set\n",
        "import scikitplot as skplt\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import norm\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import r2_score, roc_auc_score, roc_curve\n",
        "from scipy import stats  # For in-built method to get PCC\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.svm import SVC\n",
        "#importing libraries\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "import random\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import BatchNormalization,Activation,Dropout,LSTM\n",
        "import copy\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import joblib\n",
        "import scipy.signal as sg\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from spafe.features.lpc import lpc, lpcc\n",
        "#from hmmlearn import hmm\n",
        "from spafe.features.rplp import rplp, plp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import r2_score, roc_auc_score, roc_curve\n",
        "from scipy import stats  # For in-built method to get PCC\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "set(style='white', context='notebook', palette='deep')\n",
        "\n",
        "\n",
        "class feature_calculation():\n",
        "    def mfcc_feature(self, audio, sample_rate):\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "\n",
        "        return mfcc # it returns a np.array with size (40,'n') where n is the number of audio frames.\n",
        "\n",
        "    def RMS_feature(self, audio, sample_rate):\n",
        "        rms = librosa.feature.rms(y=audio)\n",
        "\n",
        "        return rms # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def CEN_feature(self, audio, sample_rate):\n",
        "        cen = librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
        "\n",
        "        return cen  # it returns a np.array with size (12,'n') where n is the number of audio frames.\n",
        "\n",
        "    def melspectrogram_feature(self, audio, sample_rate):\n",
        "        melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return melspectrogram # it returns a np.array with size (128,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_centroid_feature(self, audio, sample_rate):\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return spectral_centroid  # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def tonnetz_feature(self, audio, sample_rate):\n",
        "        y = librosa.effects.harmonic(audio)\n",
        "        tonnetz = librosa.feature.tonnetz(y=y, sr=sample_rate)\n",
        "\n",
        "        return tonnetz # it returns a np.array with size (6,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_contrast_feature(self, audio, sample_rate):\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return spectral_contrast # it returns a  np.array with size (7,'n') where n is the number of audio frames.\n",
        "\n",
        "    def poly_feature(self, audio, sample_rate):\n",
        "        poly_features = librosa.feature.poly_features(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return poly_features  # it returns a np.array with size (2,'n') where n is the number of audio frames.\n",
        "\n",
        "    def spectral_rolloff_feature(self, audio, sample_rate):\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.95)\n",
        "\n",
        "        return spectral_rolloff  # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def chroma_stft_feature(self, audio, sample_rate):\n",
        "        chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sample_rate, n_fft=2048)\n",
        "\n",
        "        return chroma_stft  # it returns a np.array with size (12,'n') where n is the number of audio frames.\n",
        "\n",
        "    def zero_crossing_rate_features(self, audio, sample_rate):\n",
        "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
        "\n",
        "        return zero_crossing_rate # it returns a np.array with size (1,'n') where n is the number of audio frames.\n",
        "\n",
        "    def lpcc_feature(self, audio, sample_rate):\n",
        "        # compute lpccs\n",
        "        lifter = 0\n",
        "        normalize = True\n",
        "        lpccs = lpcc(sig=audio, fs=sample_rate, num_ceps=13, lifter=lifter, normalize=normalize)\n",
        "        return lpccs  # it returns a np.array with size ('n',13) where n is the number of audio frames.\n",
        "\n",
        "    def RPLP_feature(self, audio, sample_rate):\n",
        "        num_ceps = 13\n",
        "        # compute features\n",
        "        rplps = rplp(audio, sample_rate, num_ceps)\n",
        "        return rplps # it returns a np.array with size ('n',13) where n is the number of audio frames.\n",
        "\n",
        "    def pitch_feature(self, audio, sample_rate):\n",
        "        pitches, magnitudes = librosa.core.piptrack(audio, sr=16000, fmin=75, fmax=1600)\n",
        "        return pitches[:200,:]  # it returns a np.array with size (200,'n') where n is the number of audio frames.\n",
        "\n",
        "\n",
        "class feature_analysis_graphs():\n",
        "    def sample_graph(self, samples, sample_rate):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        librosa.display.waveplot(samples, sr=sample_rate)\n",
        "        ax.label_outer()\n",
        "        ax.set(title='Data Respresentation')\n",
        "        plt.show()\n",
        "\n",
        "    def MFCC_graph(self, samples):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(samples, x_axis='time', ax=ax)\n",
        "        ax.set(title='MFCC')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def melspectrogram_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        S_dB = librosa.power_to_db(data, ref=np.max)\n",
        "        img = librosa.display.specshow(S_dB, x_axis='time',\n",
        "                                       y_axis='mel', sr=16000,\n",
        "                                       fmax=8000, ax=ax)\n",
        "        ax.set(title='Mel-frequency spectrogram')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def poly_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data[1].T, alpha=0.8, label='Poly Feature')\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def zero_crossing_rate_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data[0], label='zero crossing rate')\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def root_mean_square_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.semilogy(times, data[0], label='RMS Energy')\n",
        "        ax.set(xticks=[])\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def Chroma_Energy_Normalized_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(data, y_axis='chroma', x_axis='time', ax=ax)\n",
        "        ax.set(title='chroma_cen')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def Spectral_Centroid_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data.T, label='Spectral Centroid')\n",
        "        ax.legend(loc='upper right')\n",
        "        ax.set(title='log Power spectrogram')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def Tonal_Centroid_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(data, y_axis='tonnetz', x_axis='time', ax=ax)\n",
        "        ax.set(title='Tonal Centroids (Tonnetz)')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def spectral_contrast_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data[0], label='Spectral Contrast')\n",
        "        ax.set(ylabel='Hz', xticks=[], xlim=[times.min(), times.max()])\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def roll_off_frequency_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        times = librosa.times_like(data)\n",
        "        ax.plot(times, data[0], label='roll-off frequency')\n",
        "        ax.legend()\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def Chroma_stft_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(data, y_axis='chroma', x_axis='time', ax=ax)\n",
        "        ax.set(title='Chromagram')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def lpcc_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "        img = librosa.display.specshow(data.T, y_axis='chroma', x_axis='time', ax=ax)\n",
        "        ax.set(title='LPCC')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def rplp_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(data.T, x_axis='time', ax=ax)\n",
        "        ax.set(title='Rasta PLP')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "    def pitch_graph(self, data):\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        img = librosa.display.specshow(data, x_axis='time', ax=ax)\n",
        "        ax.set(title='Pitch')\n",
        "        ax.label_outer()\n",
        "        plt.show()\n",
        "\n",
        "# config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
        "# matplotlib inline\n",
        "class Distant_Speech_Recognition():\n",
        "    def __init__(self, train_path=\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\", test_path=\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\",data_path=\"/content/drive/MyDrive/Datafiles/\",models_path=\"/content/drive/MyDrive/Models/\"):\n",
        "        self.Train = train_path\n",
        "        self.Test = test_path\n",
        "        self.models_path=models_path\n",
        "        self.data_path=data_path\n",
        "\n",
        "    def data_acquisition(self, filenames, sample_rate):\n",
        "        # Data Acquisition\n",
        "        sample, sample_rate = librosa.load(filenames, sr=sample_rate)\n",
        "        return sample, sample_rate\n",
        "\n",
        "    def speech_preprocessing(self, sample, sample_rate):\n",
        "        \"\"\"# **Emphasising** preEmphasis, smaller frequency to higher from y(t)=x(t)−αx(t−1\"\"\"\n",
        "        pre_emphasis = 1\n",
        "        emphasized_signal = np.append(sample[0], sample[1:] - pre_emphasis * sample[:-1])\n",
        "        Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "        # Remove silence\n",
        "        y = librosa.effects.split(emphasized_signal, top_db=30)\n",
        "        l = []\n",
        "        for i in y:\n",
        "            l.append(emphasized_signal[i[0]:i[1]])\n",
        "        emphasized_signal = np.concatenate(l, axis=0)\n",
        "        Time = np.linspace(0, len(emphasized_signal) / sample_rate, num=len(emphasized_signal))\n",
        "        b, a = sg.butter(4, 1000. / (sample_rate / 2.), 'high')\n",
        "        emphasized_signal_fil = sg.filtfilt(b, a, emphasized_signal)\n",
        "        # fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
        "        # ax.plot(Time, emphasized_signal, lw=1)\n",
        "        # ax.plot(Time, emphasized_signal_fil, lw=1)\n",
        "        return emphasized_signal_fil\n",
        "\n",
        "    def feature_calculation(self, sample, sample_rate):\n",
        "        features = feature_calculation()\n",
        "        sample=self.speech_preprocessing(sample, sample_rate)\n",
        "        mfcc_feature = features.mfcc_feature(sample, sample_rate)\n",
        "        RMS_feature = features.RMS_feature(sample, sample_rate)\n",
        "        lpcc_feature = features.lpcc_feature(sample, sample_rate)\n",
        "        chroma_stft_feature = features.chroma_stft_feature(sample, sample_rate)\n",
        "        melspectrogram_feature = features.melspectrogram_feature(sample, sample_rate)\n",
        "        pitch_feature = features.pitch_feature(sample, sample_rate)\n",
        "        poly_feature = features.poly_feature(sample, sample_rate)\n",
        "        RPLP_feature = features.RPLP_feature(sample, sample_rate)\n",
        "        spectral_centroid_feature = features.spectral_centroid_feature(sample, sample_rate)\n",
        "        spectral_contrast_feature = features.spectral_contrast_feature(sample, sample_rate)\n",
        "        spectral_rolloff_feature = features.spectral_rolloff_feature(sample, sample_rate)\n",
        "        tonnetz_feature = features.tonnetz_feature(sample, sample_rate)\n",
        "        zero_crossing_rate_features = features.zero_crossing_rate_features(sample, sample_rate)\n",
        "        CEN_features = features.CEN_feature(sample, sample_rate)\n",
        "        return mfcc_feature, RMS_feature, lpcc_feature, chroma_stft_feature, melspectrogram_feature, pitch_feature, poly_feature, RPLP_feature, spectral_centroid_feature, spectral_contrast_feature, spectral_rolloff_feature, tonnetz_feature, zero_crossing_rate_features, CEN_features\n",
        "    # Remove mean and normalize each column of MFCC\n",
        "    def preprocess_mfcc(self,mfcc):\n",
        "        mfcc_cp = copy.deepcopy(mfcc)\n",
        "        for i in range(mfcc.shape[1]):\n",
        "            mfcc_cp[:, i] = mfcc[:, i] - np.mean(mfcc[:, i])\n",
        "            mfcc_cp[:, i] = mfcc_cp[:, i] / np.max(np.abs(mfcc_cp[:, i]))\n",
        "        return mfcc_cp\n",
        "\n",
        "    def feature_extraction(self):\n",
        "        MainFolder = self.Train\n",
        "        Labels = os.listdir(MainFolder)\n",
        "        print(Labels)\n",
        "        Labels=Labels[:-1]\n",
        "        print(Labels)\n",
        "        filepath = []\n",
        "        train_Labels = []\n",
        "        i = 0\n",
        "        #Features_data = pd.DataFrame(columns=['features', 'class'])\n",
        "        Featured_data = pd.DataFrame(\n",
        "           columns=[ 'MFCC', 'RMS', 'CENS', 'MSS', 's_centroid', 'tonnetz', 's_contrast', 'poly', 'ROF', 'CS', 'ZCR', 'pitch', 'LPCC', 'RPLP','Class'])\n",
        "        Featured_train_data = pd.DataFrame(\n",
        "           columns=[ 'features','Class'])\n",
        "        Sample_data = pd.DataFrame(\n",
        "            columns=['samples', 'Class'])\n",
        "        Featured_corr_data = pd.DataFrame(columns=['MFCC', 'Root-mean-square','CENS','Mel-scaled-spectrogram','Spectral-Centroid','tonnetz',\n",
        "                                      'Spectral-Contrast','Poly','roll-off-frequency','Chroma_stft','ZCR','pitch','LPCC','RPLP','class'])\n",
        "        sample_rate = 16000\n",
        "        for Label in Labels:\n",
        "            print(Label)\n",
        "            SubFolder = MainFolder +\"/\"+ Label + \"/\"\n",
        "            # print(SubFolder)\n",
        "            for filename in glob.glob(os.path.join(SubFolder, '*.wav')):\n",
        "                train_Labels.append(Label)\n",
        "                sample, sample_rate = self.data_acquisition(filename, sample_rate)\n",
        "                mfcc_feature, RMS_feature, lpcc_feature, chroma_stft_feature, melspectrogram_feature, pitch_feature, poly_feature, RPLP_feature, spectral_centroid_feature, spectral_contrast_feature, spectral_rolloff_feature, tonnetz_feature, zero_crossing_rate_feature, CEN_features = self.feature_calculation(\n",
        "                    sample, sample_rate)\n",
        "                # normalizing\n",
        "                # Convert .wave into array\n",
        "                # Extract Feautures\n",
        "\n",
        "                MFCC = mfcc_feature.flatten()\n",
        "                RMS = RMS_feature.flatten()\n",
        "                CENS = CEN_features.flatten()\n",
        "                MSS = melspectrogram_feature.flatten()\n",
        "                s_centroid = spectral_centroid_feature.flatten()\n",
        "                tonnetz = tonnetz_feature.flatten()\n",
        "                s_contrast = spectral_contrast_feature.flatten()\n",
        "                poly = poly_feature.flatten()\n",
        "                ROF = spectral_rolloff_feature.flatten()\n",
        "                CS = chroma_stft_feature.flatten()\n",
        "                ZCR = zero_crossing_rate_feature.flatten()\n",
        "                pitch = pitch_feature.flatten()\n",
        "                LPCC = lpcc_feature.flatten()\n",
        "                RPLP = RPLP_feature.flatten()\n",
        "                # normalizing\n",
        "\n",
        "                features = np.concatenate(( MFCC,RPLP ,MSS, poly, ZCR))\n",
        "\n",
        "                # padding and trimming\n",
        "                max_len = 6000\n",
        "\n",
        "                pad_width = max_len - features.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "\n",
        "                features = features[:max_len]\n",
        "\n",
        "                Featured_train_data.loc[i] =[features, Label]\n",
        "                MFCC,RMS, CENS, MSS, s_centroid, tonnetz , s_contrast, poly, ROF, CS, ZCR, pitch, LPCC, RPLP =  pd.Series(MFCC) , pd.Series(RMS), pd.Series(CENS), pd.Series(MSS), pd.Series(s_centroid), pd.Series(tonnetz) , pd.Series(s_contrast) ,pd.Series(poly), pd.Series(ROF), pd.Series(CS), pd.Series(ZCR), pd.Series(pitch), pd.Series(LPCC), pd.Series(RPLP)\n",
        "                samples=  pd.Series(sample)\n",
        "\n",
        "                Featured_corr_data.loc[i] = [ MFCC.corr(samples) ,RMS.corr(samples) ,CENS.corr(samples), MSS.corr(samples), s_centroid.corr(samples), tonnetz.corr(samples), s_contrast.corr(samples), poly.corr(samples), ROF.corr(samples), CS.corr(samples),ZCR.corr(samples), pitch.corr(samples), LPCC.corr(samples), RPLP.corr(samples), Label]\n",
        "\n",
        "                #features = np.concatenate((MFCC, RMS, CENS, MSS, s_centroid, tonnetz, s_contrast, poly, ROF, CS, ZCR, pitch, LPCC, RPLP))\n",
        "                #print(features)\n",
        "                #print(features.shape)\n",
        "                # padding and trimming\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - MFCC.shape[0]\n",
        "                if pad_width > 0:\n",
        "                    MFCC = np.pad(MFCC, pad_width=((0, pad_width)), mode='constant')\n",
        "                MFCC = MFCC[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - RMS.shape[0]\n",
        "                if pad_width > 0:\n",
        "                    RMS = np.pad(RMS, pad_width=((0, pad_width)), mode='constant')\n",
        "                RMS = RMS[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - CENS.shape[0]\n",
        "                if pad_width > 0:\n",
        "                    CENS = np.pad(CENS, pad_width=((0, pad_width)), mode='constant')\n",
        "                CENS = CENS[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - MSS.shape[0]\n",
        "                if pad_width > 0:\n",
        "                    MSS = np.pad(MSS, pad_width=((0, pad_width)), mode='constant')\n",
        "                MSS = MSS[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - s_centroid.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   s_centroid = np.pad(s_centroid, pad_width=((0, pad_width)), mode='constant')\n",
        "                s_centroid = s_centroid[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - tonnetz.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   tonnetz = np.pad(tonnetz, pad_width=((0, pad_width)), mode='constant')\n",
        "                tonnetz = tonnetz[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - s_contrast.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   s_contrast = np.pad(s_contrast, pad_width=((0, pad_width)), mode='constant')\n",
        "                s_contrast = s_contrast[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - poly.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   poly = np.pad(poly, pad_width=((0, pad_width)), mode='constant')\n",
        "                poly = poly[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - ROF.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   ROF = np.pad(ROF, pad_width=((0, pad_width)), mode='constant')\n",
        "                ROF = ROF[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - CS.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   CS = np.pad(CS, pad_width=((0, pad_width)), mode='constant')\n",
        "                CS = CS[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - ZCR.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   ZCR = np.pad(ZCR, pad_width=((0, pad_width)), mode='constant')\n",
        "                ZCR = ZCR[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - pitch.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   pitch = np.pad(pitch, pad_width=((0, pad_width)), mode='constant')\n",
        "                pitch = pitch[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - LPCC.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   LPCC = np.pad(LPCC, pad_width=((0, pad_width)), mode='constant')\n",
        "                LPCC = LPCC[:max_len]\n",
        "                max_len = 2000\n",
        "                pad_width = max_len - RPLP.shape[0]\n",
        "                if pad_width > 0:\n",
        "                   RPLP = np.pad(RPLP, pad_width=((0, pad_width)), mode='constant')\n",
        "                RPLP = RPLP[:max_len]\n",
        "                #print(features.shape)\n",
        "                #print(features)\n",
        "                Featured_data.loc[i] = [MFCC, RMS, CENS, MSS, s_centroid, tonnetz, s_contrast, poly, ROF, CS, ZCR, pitch, LPCC, RPLP, Label]\n",
        "\n",
        "                # padding and trimming\n",
        "                max_len = 6000\n",
        "\n",
        "                pad_width = max_len - sample.shape[0]\n",
        "                if pad_width > 0:\n",
        "                    sample = np.pad(sample, pad_width=((0, pad_width)), mode='constant')\n",
        "\n",
        "                sample = sample[:max_len]\n",
        "\n",
        "                Sample_data.loc[i] = [sample, Label]\n",
        "                i += 1\n",
        "        # print(filepath)\n",
        "        # inialize labelsa\n",
        "        sample_rate = 16000\n",
        "        Featured_train_data.to_csv(self.data_path+'Featured_train_data.csv')\n",
        "        #Featured_data.to_csv(self.data_path+'Featured_data.csv')\n",
        "        #Sample_data.to_csv(self.data_path+'Sample_data.csv')\n",
        "        #Featured_corr_data.to_csv(self.data_path+'Featured_corr_data.csv')\n",
        "        self.trainfilepath = filepath\n",
        "        self.Labels = Labels\n",
        "        self.train_Featured_data = Featured_data\n",
        "        self.train_Labels = train_Labels\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def feature_selection(self):\n",
        "        rs= pd.read_csv(r'/content/drive/MyDrive/Datafiles/Featured_corr_data.csv',encoding='utf-8')\n",
        "        # separate variables into new data frames\n",
        "        train=pd.DataFrame(rs)\n",
        "        le = LabelEncoder()\n",
        "        train['class'] = le.fit_transform(train['class'].tolist())\n",
        "        train = train.fillna(0)\n",
        "        train.head()\n",
        "        cor_matrix = train.corr()\n",
        "        cor_matrix.head()\n",
        "        fig, ax = plt.subplots(figsize=(11, 11))\n",
        "        ax = heatmap(cor_matrix)\n",
        "        plt.show()\n",
        "        # Correlation with output variable\n",
        "        cor_target = abs(cor_matrix['class'])\n",
        "\n",
        "        # Selecting highly correlated features\n",
        "        relevant_features = cor_target[cor_target > 0.12]\n",
        "        print(\"Most Correlated:\",relevant_features)\n",
        "        self.mostcorrfeatures=relevant_features\n",
        "        return relevant_features\n",
        "\n",
        "    def feature_analysis_graph(self, filename):\n",
        "        #train = self.\n",
        "        sample_rate = 16000\n",
        "        sample, sample_rate = self.data_acquisition(filename, sample_rate)\n",
        "        mfcc_feature, RMS_feature, lpcc_feature, chroma_stft_feature, melspectrogram_feature, pitch_feature, poly_feature, RPLP_feature, spectral_centroid_feature, spectral_contrast_feature, spectral_rolloff_feature, tonnetz_feature, zero_crossing_rate_feature, CEN_features = self.feature_calculation(\n",
        "            sample, sample_rate)\n",
        "        feature_analysis = feature_analysis_graphs()\n",
        "        feature_analysis.sample_graph(sample, sample_rate)\n",
        "        emphasized_signal_fil=self.speech_preprocessing(sample, sample_rate)\n",
        "        feature_analysis.sample_graph(emphasized_signal_fil, sample_rate)\n",
        "        feature_analysis.MFCC_graph(mfcc_feature)\n",
        "        feature_analysis.melspectrogram_graph(melspectrogram_feature)\n",
        "        feature_analysis.poly_graph(poly_feature)\n",
        "        feature_analysis.zero_crossing_rate_graph(zero_crossing_rate_feature)\n",
        "        feature_analysis.root_mean_square_graph(RMS_feature)\n",
        "        feature_analysis.Chroma_Energy_Normalized_graph(CEN_features)\n",
        "        feature_analysis.Spectral_Centroid_graph(spectral_centroid_feature)\n",
        "        feature_analysis.Tonal_Centroid_graph(tonnetz_feature)\n",
        "        feature_analysis.spectral_contrast_graph(spectral_contrast_feature)\n",
        "        feature_analysis.roll_off_frequency_graph(spectral_rolloff_feature)\n",
        "        feature_analysis.Chroma_stft_graph(chroma_stft_feature)\n",
        "        feature_analysis.lpcc_graph(lpcc_feature)\n",
        "        feature_analysis.rplp_graph(RPLP_feature)\n",
        "        feature_analysis.pitch_graph(pitch_feature)\n",
        "\n",
        "    def feature_transformation(self):\n",
        "        #feature_selection()\n",
        "        train = pd.read_csv(self.data_path+'/Featured_train_data.csv')\n",
        "        feature=np.array(train['features'].tolist())\n",
        "        target = train[\"Class\"]\n",
        "        print(type(train[\"features\"]), type(train[\"Class\"]))\n",
        "        #print(type(feature_test), type(target_test))\n",
        "        # converting labels into numeric\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        target=le.fit_transform(target)\n",
        "        feature_train, feature_test, target_train, target_test = train_test_split(feature,target)\n",
        "        print(type(feature_train), type(target_train))\n",
        "        print(type(feature_test), type(target_test))\n",
        "        return feature_train,feature_test,target_train,target_test\n",
        "    def Randomforest_model(self,feature_train,target_train,feature_test,target_test):\n",
        "        # Create a Gaussian Classifier\n",
        "        clff = RandomForestClassifier(n_estimators=800)\n",
        "        # Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "        clff = clff.fit(feature_train, target_train)\n",
        "        y_predd = clff.predict(feature_test)\n",
        "        # Model Accuracy, how often is the classifier correct?\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(target_test, y_predd))\n",
        "        heatmap(confusion_matrix(target_test, y_predd), annot=True, cmap='Blues')\n",
        "        plt.show()\n",
        "        model_path = self.models_path\n",
        "        joblib.dump(clff, model_path + \"model_3000.sav\")\n",
        "        return target_test, y_predd\n",
        "\n",
        "    def k_nearest_neighbor_model(self,feature_train,target_train,feature_test,target_test):\n",
        "        # Create a KNN Classifier\n",
        "\n",
        "        knn = KNeighborsClassifier()\n",
        "        # Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "        knn = knn.fit(feature_train, target_train)\n",
        "        y_predd2 = knn.predict(feature_test)\n",
        "        # Model Accuracy, how often is the classifier correct?\n",
        "        print(\"Accuracy KNN:\", metrics.accuracy_score(target_test, y_predd2))\n",
        "        heatmap(confusion_matrix(target_test, y_predd2), annot=True, cmap='Blues')\n",
        "        plt.show()\n",
        "        model_path = self.models_path\n",
        "        joblib.dump(knn, model_path + \"model_knn.sav\")\n",
        "        return target_test, y_predd2\n",
        "\n",
        "    def svm(self,feature_train,target_train,feature_test,target_test):\n",
        "        svm_model_linear = SVC(kernel='linear', C=1).fit(feature_train, target_train)\n",
        "        y_predd3 = svm_model_linear.predict(feature_test)\n",
        "\n",
        "        # model accuracy for X_test\n",
        "        accuracy = svm_model_linear.score(feature_test, target_test)\n",
        "        print(\"Accuracy SVM:\", metrics.accuracy_score(target_test, y_predd3))\n",
        "        heatmap(confusion_matrix(target_test, y_predd3), annot=True, cmap='Blues')\n",
        "        plt.show()\n",
        "        # creating a confusion matrix\n",
        "        # cm = confusion_matrix(target_test, svm_predictions)feature_test, target_test\n",
        "        model_path = self.models_path\n",
        "        joblib.dump(svm_model_linear, model_path + \"model_svm.sav\")\n",
        "        return target_test,y_predd3\n",
        "    def feature_transformation_shaping(self,features,max_len,Features_data,i,Label):\n",
        "        # padding and trimming\n",
        "        pad_width = max_len - features.shape[0]\n",
        "        if pad_width > 0:\n",
        "          features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "        features = features[:max_len]\n",
        "        Features_data.loc[i] = [features, Label]\n",
        "        return Features_data\n",
        "\n",
        "    def results(self,target_test, predicted_test,ModelName,labels):\n",
        "       target_names = labels\n",
        "       print(classification_report(target_test, predicted_test, target_names=target_names))\n",
        "       y_test = target_test\n",
        "       preds = predicted_test\n",
        "       rms = np.sqrt(np.mean(np.power((np.array(y_test) - np.array(preds)), 2)))\n",
        "       score = r2_score(y_test, preds)\n",
        "       mae = mean_absolute_error(y_test, preds)\n",
        "       mse = mean_squared_error(y_test, preds)\n",
        "       pearson_coef, p_value = stats.pearsonr(y_test, preds)\n",
        "\n",
        "       print(\"root mean square:\", rms)\n",
        "       print(\"score:\", score)\n",
        "       print(\"mean absolute error:\", mae)\n",
        "       print(\"mean squared error:\", mse)\n",
        "       print(\"pearson_coef:\", pearson_coef)\n",
        "       print(\"p_value:\", p_value)\n",
        "       print(\"=======================================================================\\n\\n\")\n",
        "       skplt.metrics.plot_confusion_matrix(\n",
        "        y_test,\n",
        "        preds,\n",
        "        figsize=(10, 6), title=\"Confusion matrix\\n Deposite Category \"+ModelName)\n",
        "       plt.xlim(-0.5, len(np.unique(y_test)) - 0.5)\n",
        "       plt.ylim(len(np.unique(y_test)) - 0.5, -0.5)\n",
        "       plt.savefig('cvroc.png')\n",
        "       plt.show()\n",
        "       # Bagging\n",
        "       ns_probs = [0 for _ in range(len(y_test))]\n",
        "       #lr_probs = predictions\n",
        "       #best_found_fina=individual_list\n",
        "       ns_aucb = roc_auc_score(y_test, ns_probs)\n",
        "       lr_aucb = roc_auc_score(y_test, preds)\n",
        "       precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "       no_skill = len(y_test[y_test==1]) / len(y_test)\n",
        "       plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "       plt.plot(precision, recall,thresholds, marker='.', label='Logistic')\n",
        "       # axis labels\n",
        "       plt.xlabel('Recall')\n",
        "       plt.ylabel('Precision')\n",
        "       # show the legend\n",
        "       plt.legend()\n",
        "       # show the plot\n",
        "       plt.show()\n",
        "       # summarize scores\n",
        "       print('No Skill: ROC AUC=%.3f' % (ns_aucb))\n",
        "       print('Logistic: ROC AUC=%.3f' % (lr_aucb))\n",
        "       # calculate roc curves\n",
        "       ns_fprb, ns_tprb, _ = roc_curve(y_test, ns_probs)\n",
        "       lr_fprb, lr_tprb, _ = roc_curve(y_test, preds)\n",
        "       # plot the roc curve for the model\n",
        "       plt.plot(ns_fprb, ns_tprb, linestyle='--', label='No Skill')\n",
        "       plt.plot(lr_fprb, lr_tprb, marker='.', label='target')\n",
        "       plt.plot(lr_fprb, lr_tprb, marker='.', label= ModelName)\n",
        "       # axis labels\n",
        "       plt.xlabel('False Positive Rate')\n",
        "       plt.ylabel('True Positive Rate')\n",
        "       # show the legend\n",
        "       plt.legend()\n",
        "       plt.title(\"ROC Curve Graph\")\n",
        "       plt.savefig('comparisonroc.png')\n",
        "       # show the plot\n",
        "       plt.show()\n",
        "\n",
        "    def MLfunction(self,feature,target,featurecomboname,labels):\n",
        "        # converting labels into numeric\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        target=le.fit_transform(target)\n",
        "        # features = preprocessing.MinMaxScaler().fit_transform(features)\n",
        "        feature_train, feature_test, target_train, target_test = train_test_split(feature,target)\n",
        "        #Create a Gaussian Classifier\n",
        "        clff=RandomForestClassifier(n_estimators=800)\n",
        "        #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "        clff = clff.fit(feature_train,target_train)\n",
        "        y_predd1=clff.predict(feature_test)\n",
        "        # Model Accuracy, how often is the classifier correct?\n",
        "        #print(\"Random Forest Accuracy:\",metrics.accuracy_score(target_test, y_predd1))\n",
        "        modelname= featurecomboname+\"Random Forest Classifier\"\n",
        "        self.results(target_test, y_predd1,modelname,labels)\n",
        "        model_path = '/content/drive/MyDrive/Models/'\n",
        "        modelfilename=featurecomboname+\"model_rfverynoisy.sav\"\n",
        "        joblib.dump(clff, model_path+modelfilename)\n",
        "        #Create a KNN Classifier\n",
        "        knn=KNeighborsClassifier()\n",
        "        #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "        knn = knn.fit(feature_train,target_train)\n",
        "        y_predd2=knn.predict(feature_test)\n",
        "        # Model Accuracy, how often is the classifier correct?\n",
        "        #print(\"Accuracy KNN:\",metrics.accuracy_score(target_test, y_predd2))\n",
        "        modelname= featurecomboname+\"KNN Classifier\"\n",
        "        self.results(target_test, y_predd2,\"KNN Classifier\",labels)\n",
        "        model_path = '/content/drive/MyDrive/Models/'\n",
        "        modelfilename=featurecomboname+\"model_knnverynoisy.sav\"\n",
        "        joblib.dump(knn, model_path+modelfilename)\n",
        "        # training a linear SVM classifier\n",
        "        svm_model_linear = SVC(kernel = 'linear', C = 1).fit(feature_train,target_train)\n",
        "        y_predd3 = svm_model_linear.predict(feature_test)\n",
        "        # model accuracy for X_test\n",
        "        accuracy = svm_model_linear.score(feature_test, target_test)\n",
        "        #print(\"Accuracy SVM:\",metrics.accuracy_score(target_test, y_predd3))\n",
        "        modelname= featurecomboname+\"SVM Classifier\"\n",
        "        self.results(target_test, y_predd3,modelname,labels)\n",
        "        model_path = '/content/drive/MyDrive/Models/'\n",
        "        modelfilename=featurecomboname+\"model_svmverynoisy.sav\"\n",
        "        joblib.dump(svm_model_linear, model_path+modelfilename)\n",
        "        model1 = RandomForestClassifier()\n",
        "        model2 = KNeighborsClassifier()\n",
        "        model3 = LogisticRegression()\n",
        "        Voting = VotingClassifier(estimators=[('RF', model1 ), ('knn', model2),('lr',model3)], voting='hard')\n",
        "        Voting.fit(feature_train,target_train)\n",
        "        vpredictions = Voting.predict(feature_test)\n",
        "        vscore = Voting.score(feature_test, target_test)\n",
        "        #print(\"Voting Score\", vscore)\n",
        "        modelname= featurecomboname+\"Voting Classifier\"\n",
        "        self.results(target_test, vpredictions,modelname,labels)\n",
        "        model_path = '/content/drive/MyDrive/Models/'\n",
        "        modelfilename=featurecomboname+\"model_votingverynoisy.sav\"\n",
        "        joblib.dump(Voting, model_path+modelfilename)\n",
        "        # print(\"predicition:\", vpredictions)\n",
        "    def ML_model_implimentation(self,no_of_classes,no_of_samples):\n",
        "        warnings.filterwarnings('ignore')\n",
        "        features_cal = feature_calculation()\n",
        "      # setting the path where all file's folder are\n",
        "        root = self.Train\n",
        "        Features_data = pd.DataFrame(columns=['features','class'])\n",
        "        MFCC_features=pd.DataFrame(columns=['features','class'])\n",
        "        MSS_features=pd.DataFrame(columns=['features','class'])\n",
        "        poly_features=pd.DataFrame(columns=['features','class'])\n",
        "        ZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "\n",
        "        MFCCMSS_features=pd.DataFrame(columns=['features','class'])\n",
        "        MFCCpoly_features=pd.DataFrame(columns=['features','class'])\n",
        "        MFCCZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "\n",
        "        MSSpoly_features=pd.DataFrame(columns=['features','class'])\n",
        "        MSSZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "        polyZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "\n",
        "        MFCCMSSpoly_features=pd.DataFrame(columns=['features','class'])\n",
        "        MFCCpolyZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "        MSSpolyZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "        all_wave = pd.DataFrame(columns=['features','class'])\n",
        "        all_label = []\n",
        "        i = 0\n",
        "        max_len = 8000\n",
        "        sample_rate = 16000\n",
        "        no_of_samples = 300\n",
        "        MainFolder=\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\"\n",
        "        labels=os.listdir(MainFolder)\n",
        "\n",
        "        # Loading the features in the dataframe\n",
        "        for label in labels[:no_of_classes]:\n",
        "          print(\"Class Folder \"+label)\n",
        "          folders = os.path.join(root,label)\n",
        "          items = os.listdir(folders)\n",
        "          if no_of_samples == 0:\n",
        "            no_of_samples= len(items)\n",
        "          for item in items[:no_of_samples]:\n",
        "            #print(\"audio name\"+item)\n",
        "            path = os.path.join(folders,item)\n",
        "            #Convert .wave into array\n",
        "            samples, sample_rate=librosa.load(path ,sr=sample_rate)\n",
        "            #Extract Feautures\n",
        "            #samples=self.speech_preprocessing( samples, sample_rate)\n",
        "            MFCC = features_cal.mfcc_feature(samples , sample_rate)\n",
        "            #RPLP=features_cal.RPLP_feature(samples , sample_rate)\n",
        "            MSS = features_cal.melspectrogram_feature(samples , sample_rate)\n",
        "            poly = features_cal.poly_feature(samples , sample_rate)\n",
        "            ZCR = features_cal.zero_crossing_rate_features(samples , sample_rate)\n",
        "            # flatten an array\n",
        "            MFCC = MFCC.flatten()\n",
        "            #RPLP=RPLP.flatten()\n",
        "            MSS = MSS.flatten()\n",
        "            poly = poly.flatten()\n",
        "            ZCR = ZCR.flatten()\n",
        "            # normalizing\n",
        "            # MFCC = normalize(MFCC)\n",
        "            features = np.concatenate(( MFCC,MSS, poly, ZCR))\n",
        "            features1=MFCC\n",
        "            features2=MSS\n",
        "            features3=poly\n",
        "            features4=ZCR\n",
        "\n",
        "            features_combo1 = np.concatenate(( MFCC,MSS))\n",
        "            features_combo2 = np.concatenate(( MFCC,poly))\n",
        "            features_combo3 = np.concatenate(( MFCC,ZCR))\n",
        "            features_combo4 = np.concatenate(( MSS, poly))\n",
        "            features_combo5 = np.concatenate(( MSS,ZCR))\n",
        "            features_combo6 = np.concatenate(( poly,ZCR))\n",
        "\n",
        "            features_combo7 = np.concatenate(( MFCC,MSS, poly))\n",
        "            features_combo8 = np.concatenate(( MFCC,poly, ZCR))\n",
        "            features_combo9 = np.concatenate(( MSS, poly, ZCR))\n",
        "\n",
        "            Features_data=self.feature_transformation_shaping(features,max_len,Features_data,i,label)\n",
        "            MFCC_features=self.feature_transformation_shaping(features1,max_len,MFCC_features,i,label)\n",
        "            MSS_features=self.feature_transformation_shaping(features2,max_len,MSS_features,i,label)\n",
        "            poly_features=self.feature_transformation_shaping(features3,max_len,poly_features,i,label)\n",
        "            ZCR_features=self.feature_transformation_shaping(features4,max_len,ZCR_features,i,label)\n",
        "            MFCCMSS_features=self.feature_transformation_shaping(features_combo1,max_len,MFCCMSS_features,i,label)\n",
        "            MFCCpoly_features=self.feature_transformation_shaping(features_combo2,max_len,MFCCpoly_features,i,label)\n",
        "            MFCCZCR_features=self.feature_transformation_shaping(features_combo3,max_len,MFCCZCR_features,i,label)\n",
        "            MSSpoly_features=self.feature_transformation_shaping(features_combo4,max_len,MSSpoly_features,i,label)\n",
        "            MSSZCR_features=self.feature_transformation_shaping(features_combo5,max_len,MSSZCR_features,i,label)\n",
        "            polyZCR_features=self.feature_transformation_shaping(features_combo6,max_len,polyZCR_features,i,label)\n",
        "            MFCCMSSpoly_features=self.feature_transformation_shaping(features_combo7,max_len,MFCCMSSpoly_features,i,label)\n",
        "            MFCCpolyZCR_features=self.feature_transformation_shaping(features_combo8,max_len,MFCCpolyZCR_features,i,label)\n",
        "            MSSpolyZCR_features=self.feature_transformation_shaping(features_combo9,max_len,MSSpolyZCR_features,i,label)\n",
        "            all_wave=self.feature_transformation_shaping(samples,max_len,all_wave,i,label)\n",
        "            i=i+1\n",
        "        Features_data.to_csv(self.data_path+'All_Features_dataverynoisy.csv')\n",
        "        MFCC_features.to_csv(self.data_path+'MFCC_features_dataverynoisy.csv')\n",
        "        MSS_features.to_csv(self.data_path+'MSS_features_dataverynoisy.csv')\n",
        "        poly_features.to_csv(self.data_path+'poly_features_dataverynoisy.csv')\n",
        "        ZCR_features.to_csv(self.data_path+'ZCR_features_dataverynoisy.csv')\n",
        "        MFCCMSS_features.to_csv(self.data_path+'MFCCMSS_features_dataverynoisy.csv')\n",
        "        MFCCpoly_features.to_csv(self.data_path+'MFCCpoly_features_dataverynoisy.csv')\n",
        "        MFCCZCR_features.to_csv(self.data_path+'MFCCZCR_features_dataverynoisy.csv')\n",
        "        MSSpoly_features.to_csv(self.data_path+'MSSpoly_features_dataverynoisy.csv')\n",
        "        MSSZCR_features.to_csv(self.data_path+'MSSZCR_features_dataverynoisy.csv')\n",
        "        polyZCR_features.to_csv(self.data_path+'polyZCR_features_dataverynoisy.csv')\n",
        "        MFCCMSSpoly_features.to_csv(self.data_path+'MFCCMSSpoly_features_dataverynoisy.csv')\n",
        "        MFCCpolyZCR_features.to_csv(self.data_path+'MFCCpolyZCR_features_dataverynoisy.csv')\n",
        "        MSSpolyZCR_features.to_csv(self.data_path+'MSSpolyZCR_features_dataverynoisy.csv')\n",
        "        all_wave.to_csv(self.data_path+'all_wave_train_dataverynoisy.csv')\n",
        "        feature_all=np.array(Features_data['features'].tolist())\n",
        "        feature1=np.array(MFCC_features['features'].tolist())\n",
        "        feature2=np.array(MSS_features['features'].tolist())\n",
        "        feature3=np.array(poly_features['features'].tolist())\n",
        "        feature4=np.array(ZCR_features['features'].tolist())\n",
        "        feature_combo1=np.array(MFCCMSS_features['features'].tolist())\n",
        "        feature_combo2=np.array(MFCCpoly_features['features'].tolist())\n",
        "        feature_combo3=np.array(MFCCZCR_features['features'].tolist())\n",
        "        feature_combo4=np.array(MSSpoly_features['features'].tolist())\n",
        "        feature_combo5=np.array(MSSZCR_features['features'].tolist())\n",
        "        feature_combo6=np.array(polyZCR_features['features'].tolist())\n",
        "        feature_combo7=np.array(MFCCMSSpoly_features['features'].tolist())\n",
        "        feature_combo8=np.array(MFCCpolyZCR_features['features'].tolist())\n",
        "        feature_combo9=np.array(MSSpolyZCR_features['features'].tolist())\n",
        "        wave=np.array(all_wave['features'].tolist())\n",
        "        target = Features_data.iloc[:,-1]\n",
        "        labels=labels[:2]\n",
        "        self.MLfunction(wave,target,'All 4 Feautures',labels)\n",
        "        self.MLfunction(feature_all,target,'All 4 Feautures',labels)\n",
        "        self.MLfunction(feature1,target,'MFCC Feauture',labels)\n",
        "        self.MLfunction(feature2,target,'MSS Feauture',labels)\n",
        "        self.MLfunction(feature3,target,'Poly Feauture',labels)\n",
        "        self.MLfunction(feature4,target,'ZCR Feauture',labels)\n",
        "        self.MLfunction(feature_combo1,target,'MFCC & MSS Feautures',labels)\n",
        "        self.MLfunction(feature_combo2,target,'MFCC & poly Feautures',labels)\n",
        "        self.MLfunction(feature_combo3,target,'MFCC & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo4,target,'MSS & poly Feautures',labels)\n",
        "        self.MLfunction(feature_combo5,target,'MSS & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo6,target,'poly & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo7,target,'MFCC,MSS & poly Feautures',labels)\n",
        "        self.MLfunction(feature_combo8,target,'MFCC,poly & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo9,target,'MSS,poly & ZCR Feautures',labels)\n",
        "\n",
        "    def ML_data_model_implimentation(self):\n",
        "        Features_data= pd.read_csv(self.data_path+'All_Features_dataverynoisy.csv',encoding='utf-8')\n",
        "        Features_data.to_csv(self.data_path+'All_Features_dataverynoisy.csv')\n",
        "        MFCC_features= pd.read_csv(self.data_path+'MFCC_features_dataverynoisy.csv')\n",
        "        MSS_features= pd.read_csv(self.data_path+'MSS_features_dataverynoisy.csv')\n",
        "        poly_features= pd.read_csv(self.data_path+'poly_features_dataverynoisy.csv')\n",
        "        ZCR_features= pd.read_csv(self.data_path+'ZCR_features_dataverynoisy.csv')\n",
        "        MFCCMSS_features= pd.read_csv(self.data_path+'MFCCMSS_features_dataverynoisy.csv')\n",
        "        MFCCpoly_features= pd.read_csv(self.data_path+'MFCCpoly_features_dataverynoisy.csv')\n",
        "        MFCCZCR_features= pd.read_csv(self.data_path+'MFCCZCR_features_dataverynoisy.csv')\n",
        "        MSSpoly_features= pd.read_csv(self.data_path+'MSSpoly_features_dataverynoisy.csv')\n",
        "        MSSZCR_features= pd.read_csv(self.data_path+'MSSZCR_features_dataverynoisy.csv')\n",
        "        polyZCR_features= pd.read_csv(self.data_path+'polyZCR_features_dataverynoisy.csv')\n",
        "        MFCCMSSpoly_features= pd.read_csv(self.data_path+'MFCCMSSpoly_features_dataverynoisy.csv')\n",
        "        MFCCpolyZCR_features= pd.read_csv(self.data_path+'MFCCpolyZCR_features_dataverynoisy.csv')\n",
        "        MSSpolyZCR_features= pd.read_csv(self.data_path+'MSSpolyZCR_features_dataverynoisy.csv')\n",
        "        all_wave= pd.read_csv(self.data_path+'all_wave_train_dataverynoisy.csv')\n",
        "        feature_all=np.array(Features_data['features'].tolist())\n",
        "        feature1=np.array(MFCC_features['features'].tolist())\n",
        "        feature2=np.array(MSS_features['features'].tolist())\n",
        "        feature3=np.array(poly_features['features'].tolist())\n",
        "        feature4=np.array(ZCR_features['features'].tolist())\n",
        "        feature_combo1=np.array(MFCCMSS_features['features'].tolist())\n",
        "        feature_combo2=np.array(MFCCpoly_features['features'].tolist())\n",
        "        feature_combo3=np.array(MFCCZCR_features['features'].tolist())\n",
        "        feature_combo4=np.array(MSSpoly_features['features'].tolist())\n",
        "        feature_combo5=np.array(MSSZCR_features['features'].tolist())\n",
        "        feature_combo6=np.array(polyZCR_features['features'].tolist())\n",
        "        feature_combo7=np.array(MFCCMSSpoly_features['features'].tolist())\n",
        "        feature_combo8=np.array(MFCCpolyZCR_features['features'].tolist())\n",
        "        feature_combo9=np.array(MSSpolyZCR_features['features'].tolist())\n",
        "        wave=np.array(all_wave['features'].tolist())\n",
        "        target = Features_data.iloc[:,-1]\n",
        "        print(wave.shape)\n",
        "        labels=labels[:1]\n",
        "        self.MLfunction(wave,target,'All 4 Feautures',labels)\n",
        "        self.MLfunction(feature_all,target,'All 4 Feautures',labels)\n",
        "        self.MLfunction(feature1,target,'MFCC Feauture',labels)\n",
        "        self.MLfunction(feature2,target,'MSS Feauture',labels)\n",
        "        self.MLfunction(feature3,target,'Poly Feauture',labels)\n",
        "        self.MLfunction(feature4,target,'ZCR Feauture',labels)\n",
        "        self.MLfunction(feature_combo1,target,'MFCC & MSS Feautures',labels)\n",
        "        self.MLfunction(feature_combo2,target,'MFCC & poly Feautures',labels)\n",
        "        self.MLfunction(feature_combo3,target,'MFCC & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo4,target,'MSS & poly Feautures',labels)\n",
        "        self.MLfunction(feature_combo5,target,'MSS & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo6,target,'poly & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo7,target,'MFCC,MSS & poly Feautures',labels)\n",
        "        self.MLfunction(feature_combo8,target,'MFCC,poly & ZCR Feautures',labels)\n",
        "        self.MLfunction(feature_combo9,target,'MSS,poly & ZCR Feautures',labels)\n",
        "if __name__ == \"__main__\":\n",
        "    filename='/content/drive/MyDrive/archive (1)/augmented_dataset/'\n",
        "    Distant_Speech_Recognition = Distant_Speech_Recognition()\n",
        "    #Distant_Speech_Recognition.feature_analysis_graph(filename)\n",
        "    #Distant_Speech_Recognition.feature_extraction()\n",
        "    #Distant_Speech_Recognition.feature_selection()\n",
        "    #Distant_Speech_Recognition.feature_transformation()\n",
        "    no_of_classes=30\n",
        "    no_of_samples=0\n",
        "    Distant_Speech_Recognition.ML_model_implimentation(no_of_classes,no_of_samples)\n",
        "    #Distant_Speech_Recognition.ML_data_model_implimentation()\n",
        "    #Distant_Speech_Recognition.MLfunction()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Folder five\n",
            "Class Folder eight\n",
            "Class Folder four\n",
            "Class Folder go\n",
            "Class Folder happy\n",
            "Class Folder house\n",
            "Class Folder left\n"
          ]
        }
      ]
    }
  ]
}