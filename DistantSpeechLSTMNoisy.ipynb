{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj0mutNEz4VT"
      },
      "source": [
        "### Speech Processing Analysis & Machine Learning & Ensemble Modelling\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXCBp_7hPapa",
        "outputId": "bc6f5819-2652-4670-ed34-88ccf31b8f3f"
      },
      "source": [
        "!pip install spafe\n",
        "!pip install pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.1.2-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from spafe) (1.19.5)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.1.2\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot, install\n",
            "Successfully installed install-1.3.4 scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPWozanHPX7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb1f592-60c2-4ef9-f7d0-cff033fce5ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUxJhtRmt3fq",
        "outputId": "3970e166-713d-4744-e05d-0b131488e14d"
      },
      "source": [
        "import os\n",
        "for i in os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset/\"):\n",
        "    print(str(len(os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset/\"+i))) +\" \"+ i +\" audios\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1356 bed audios\n",
            "1357 bird audios\n",
            "1424 cat audios\n",
            "1198 down audios\n",
            "1498 dog audios\n",
            "1133 eight audios\n",
            "1092 five audios\n",
            "960 go audios\n",
            "2400 four audios\n",
            "1481 happy audios\n",
            "1505 left audios\n",
            "2392 house audios\n",
            "1253 marvel audios\n",
            "1144 nine audios\n",
            "1002 no audios\n",
            "2244 off audios\n",
            "2228 on audios\n",
            "1276 one audios\n",
            "1296 right audios\n",
            "1411 seven audios\n",
            "1485 six audios\n",
            "1463 sheila audios\n",
            "1188 three audios\n",
            "1485 stop audios\n",
            "1188 tree audios\n",
            "902 two audios\n",
            "1187 up audios\n",
            "957 wow audios\n",
            "1244 yes audios\n",
            "1306 zero audios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFvUbGcKvkhX",
        "outputId": "f4831381-1fc4-4a1c-a2ee-8b9cb96ebd06"
      },
      "source": [
        "import os\n",
        "for i in os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\"):\n",
        "    print(str(len(os.listdir(\"/content/drive/MyDrive/archive (1)/augmented_dataset_verynoisy/\"+i))) +\" \"+ i +\" audios\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1489 dog audios\n",
            "1402 cat audios\n",
            "1113 eight audios\n",
            "1223 down audios\n",
            "1112 five audios\n",
            "960 go audios\n",
            "2400 four audios\n",
            "1481 happy audios\n",
            "2392 house audios\n",
            "1485 left audios\n",
            "1253 marvel audios\n",
            "1145 nine audios\n",
            "963 no audios\n",
            "2252 off audios\n",
            "2228 on audios\n",
            "1276 one audios\n",
            "1276 right audios\n",
            "1411 seven audios\n",
            "1463 sheila audios\n",
            "1500 six audios\n",
            "1485 stop audios\n",
            "2028 three audios\n",
            "2072 tree audios\n",
            "902 two audios\n",
            "1187 up audios\n",
            "957 wow audios\n",
            "1547 yes audios\n",
            "1602 zero audios\n",
            "1367 bed audios\n",
            "1366 bird audios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhOZUt1vx9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b477af40-6310-46f6-e241-3c80669cc2e7"
      },
      "source": [
        "import os\n",
        "for i in os.listdir(\"/content/drive/MyDrive/speech_commands_v0.01/\"):\n",
        "    print(str(len(os.listdir(\"/content/drive/MyDrive/speech_commands_v0.01/\"+i))) +\" \"+ i +\" audios\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2357 five audios\n",
            "2352 eight audios\n",
            "2372 four audios\n",
            "2372 go audios\n",
            "1742 happy audios\n",
            "1750 house audios\n",
            "2353 left audios\n",
            "1746 marvin audios\n",
            "2365 nine audios\n",
            "1881 no audios\n",
            "2357 off audios\n",
            "2392 on audios\n",
            "2370 one audios\n",
            "2367 right audios\n",
            "2392 seven audios\n",
            "1734 sheila audios\n",
            "2387 six audios\n",
            "2390 stop audios\n",
            "2346 three audios\n",
            "1733 tree audios\n",
            "2373 two audios\n",
            "1745 wow audios\n",
            "2387 yes audios\n",
            "2376 up audios\n",
            "2376 zero audios\n",
            "1713 bed audios\n",
            "1731 bird audios\n",
            "1733 cat audios\n",
            "2359 down audios\n",
            "1746 dog audios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2mZyeo_PVU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8342ede1-84b3-4575-a5da-34bc2cd63470"
      },
      "source": [
        "# 1D cnn for SER\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, LSTM, Dense, Activation, Layer\n",
        "from emodata1d import load_data\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import argparse\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "def emo1d(input_shape, num_classes, args):\n",
        "    model = Sequential(name='Emo1D')\n",
        "\n",
        "    # LFLB1\n",
        "    model.add(Conv1D(filters=64, kernel_size=(3), strides=1, padding='same', data_format='channels_last',\n",
        "                     input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling1D(pool_size=4, strides=4))\n",
        "\n",
        "    # LFLB2\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling1D(pool_size=4, strides=4))\n",
        "\n",
        "    # LFLB3\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling1D(pool_size=4, strides=4))\n",
        "\n",
        "    # LFLB4\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling1D(pool_size=4, strides=4))\n",
        "\n",
        "    # LSTM\n",
        "    model.add(LSTM(units=args.num_fc,return_sequences=True))\n",
        "    model.add(SeqSelfAttention(attention_activation='tanh'))\n",
        "    model.add(LSTM(units=args.num_fc,return_sequences=False))\n",
        "\n",
        "    # FC\n",
        "    model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "    # Model compilation\n",
        "    opt = optimizers.SGD(lr=args.learning_rate, decay=args.decay, momentum=args.momentum, nesterov=True)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(model, x_tr, y_tr, x_val, y_val, args):\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)\n",
        "    mc = ModelCheckpoint('best_model.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,\n",
        "                         save_best_only=True)\n",
        "    history = model.fit(x_tr, y_tr, epochs=args.num_epochs, batch_size=args.batch_size, validation_data=(x_val, y_val),\n",
        "                        callbacks=[es, mc])\n",
        "    return model\n",
        "\n",
        "\n",
        "def test(model, x_t, y_t):\n",
        "    saved_model = load_model('best_model.h5',custom_objects={'SeqSelfAttention':SeqSelfAttention})\n",
        "    score = saved_model.evaluate(x_t, y_t, batch_size=20)\n",
        "    print(score)\n",
        "    return score\n",
        "\n",
        "\n",
        "def loadData(features,target,labels):\n",
        "  # converting labels into numeric\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    target=le.fit_transform(target)\n",
        "    # features = preprocessing.MinMaxScaler().fit_transform(features)\n",
        "    target=np_utils.to_categorical(target, num_classes=len(labels))\n",
        "    features = np.array(features).reshape(-1,features.shape[1],1)\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(np.array(features),np.array(target),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n",
        "    x_t=x_val\n",
        "    y_t=y_val\n",
        "    #x_tr, y_tr, x_t, y_t, x_val, y_val = load_data()\n",
        "\n",
        "    #x_tr = x_tr.reshape(-1, x_tr.shape[1], 1)\n",
        "    #x_t = x_t.reshape(-1, x_t.shape[1], 1)\n",
        "    #x_val = x_val.reshape(-1, x_val.shape[1], 1)\n",
        "    #y_tr = to_categorical(y_tr)\n",
        "    #y_t = to_categorical(y_t)\n",
        "    #y_val = to_categorical(y_val)\n",
        "    return x_tr, y_tr, x_t, y_t, x_val, y_val\n",
        "\n",
        "def dataspit(feature,target):\n",
        "  # converting labels into numeric\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  target=le.fit_transform(target)\n",
        "  # features = preprocessing.MinMaxScaler().fit_transform(features)\n",
        "  feature_train, feature_test, target_train, target_test = train_test_split(feature,target)\n",
        "\n",
        "def Data_preparation(train_path):\n",
        "  warnings.filterwarnings('ignore')\n",
        "  features_cal = feature_calculation()\n",
        "  # setting the path where all file's folder are\n",
        "  root = train_path\n",
        "  Features_data = pd.DataFrame(columns=['features','class'])\n",
        "  MFCC_features=pd.DataFrame(columns=['features','class'])\n",
        "  MSS_features=pd.DataFrame(columns=['features','class'])\n",
        "  poly_features=pd.DataFrame(columns=['features','class'])\n",
        "  ZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "\n",
        "  MFCCMSS_features=pd.DataFrame(columns=['features','class'])\n",
        "  MFCCpoly_features=pd.DataFrame(columns=['features','class'])\n",
        "  MFCCZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "\n",
        "  MSSpoly_features=pd.DataFrame(columns=['features','class'])\n",
        "  MSSZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "  polyZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "\n",
        "  MFCCMSSpoly_features=pd.DataFrame(columns=['features','class'])\n",
        "  MFCCpolyZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "  MSSpolyZCR_features=pd.DataFrame(columns=['features','class'])\n",
        "  all_wave = pd.DataFrame(columns=['features','class'])\n",
        "  all_label = []\n",
        "  i = 0\n",
        "  sample_rate = 16000\n",
        "  no_of_samples = 300\n",
        "  MainFolder=train_path\n",
        "  labels=os.listdir(MainFolder)\n",
        "  # Loading the features in the dataframe\n",
        "  for label in labels:\n",
        "    print(label)\n",
        "    folders = os.path.join(root,label)\n",
        "    items = os.listdir(folders)\n",
        "    for item in items:\n",
        "      path = os.path.join(folders,item)\n",
        "      #Convert .wave into array\n",
        "      samples, sample_rate=librosa.load(path ,sr=sample_rate)\n",
        "      #Extract Feautures\n",
        "      #samples=self.speech_preprocessing( samples, sample_rate)\n",
        "      MFCC = features_cal.mfcc_feature(samples , sample_rate)\n",
        "      #RPLP=features_cal.RPLP_feature(samples , sample_rate)\n",
        "      MSS = features_cal.melspectrogram_feature(samples , sample_rate)\n",
        "      poly = features_cal.poly_feature(samples , sample_rate)\n",
        "      ZCR = features_cal.zero_crossing_rate_features(samples , sample_rate)\n",
        "      # flatten an array\n",
        "      MFCC = MFCC.flatten()\n",
        "      #RPLP=RPLP.flatten()\n",
        "      MSS = MSS.flatten()\n",
        "      poly = poly.flatten()\n",
        "      ZCR = ZCR.flatten()\n",
        "      # normalizing\n",
        "      # MFCC = normalize(MFCC)\n",
        "      features = np.concatenate(( MFCC,MSS, poly, ZCR))\n",
        "      features1=MFCC\n",
        "      features2=MSS\n",
        "      features3=poly\n",
        "      features4=ZCR\n",
        "\n",
        "      features_combo1 = np.concatenate(( MFCC,MSS))\n",
        "      features_combo2 = np.concatenate(( MFCC,poly))\n",
        "      features_combo3 = np.concatenate(( MFCC,ZCR))\n",
        "      features_combo4 = np.concatenate(( MSS, poly))\n",
        "      features_combo5 = np.concatenate(( MSS,ZCR))\n",
        "      features_combo6 = np.concatenate(( poly,ZCR))\n",
        "\n",
        "      features_combo7 = np.concatenate(( MFCC,MSS, poly))\n",
        "      features_combo8 = np.concatenate(( MFCC,poly, ZCR))\n",
        "      features_combo9 = np.concatenate(( MSS, poly, ZCR))\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features = np.pad(features, pad_width=((0, pad_width)), mode='constant')\n",
        "      features = features[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features1.shape[0]\n",
        "      if pad_width > 0:\n",
        "              features1 = np.pad(features1, pad_width=((0, pad_width)), mode='constant')\n",
        "      features1 = features1[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features2.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features2 = np.pad(features2, pad_width=((0, pad_width)), mode='constant')\n",
        "      features2 = features2[:max_len]\n",
        "      # padding and trimming]\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features3.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features3 = np.pad(features3, pad_width=((0, pad_width)), mode='constant')\n",
        "      features3 = features3[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features4.shape[0]\n",
        "      if pad_width > 0:\n",
        "              features4 = np.pad(features4, pad_width=((0, pad_width)), mode='constant')\n",
        "      features4 = features4[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo1.shape[0]\n",
        "      if pad_width > 0:\n",
        "              features_combo1 = np.pad(features_combo1, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo1= features_combo1[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo2.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo2 = np.pad(features_combo2, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo2 = features_combo2[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo3.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo3 = np.pad(features_combo3, pad_width=((0, pad_width)), mode='constant')\n",
        "       features_combo3 = features_combo3[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo4.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo4 = np.pad(features_combo4, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo4 = features_combo4[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo5.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo5= np.pad(features_combo5, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo5 = features_combo5[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo6.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo6 = np.pad(features_combo6, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo6 = features_combo6[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo7.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo7 = np.pad(features_combo7, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo7 = features_combo7[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo8.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo8 = np.pad(features_combo8, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo8 = features_combo8[:max_len]\n",
        "      # padding and trimming\n",
        "      max_len = 8000\n",
        "      pad_width = max_len - features_combo9.shape[0]\n",
        "      if pad_width > 0:\n",
        "        features_combo9 = np.pad(features_combo9, pad_width=((0, pad_width)), mode='constant')\n",
        "      features_combo9 = features_combo9[:max_len]\n",
        "      if(len(samples)== 8000):\n",
        "        Featured_data.loc[i] = [features, Label]\n",
        "        MFCC_features.loc[i] = [features1, Label]\n",
        "        MSS_features.loc[i] = [features2, Label]\n",
        "        poly_features.loc[i] = [features3, Label]\n",
        "        ZCR_features.loc[i] = [features4, Label]\n",
        "        MFCCMSS_features.loc[i] = [features_combo1, Label]\n",
        "        MFCCpoly_features.loc[i] = [features_combo2, Label]\n",
        "        MFCCZCR_features.loc[i] = [features_combo3, Label]\n",
        "        MSSpoly_features.loc[i] = [features_combo4, Label]\n",
        "        MSSZCR_features.loc[i] = [features_combo5, Label]\n",
        "        polyZCR_features.loc[i] = [features_combo6, Label]\n",
        "        MFCCMSSpoly_features.loc[i] = [features_combo7, Label]\n",
        "        MFCCpolyZCR_features.loc[i] = [features_combo8, Label]\n",
        "        MSSpolyZCR_features.loc[i] = [features_combo9, Label]\n",
        "        all_wave.loc[i] = [samples, Label]\n",
        "      i += 1\n",
        "  #np.set_printoptions(threshold=sys.maxsize)\n",
        "  Features_data.to_csv(self.data_path+'All_Features_datanoisy.csv')\n",
        "  MFCC_features.to_csv(self.data_path+'MFCC_features_datanoisy.csv')\n",
        "  MSS_features.to_csv(self.data_path+'MSS_features_datanoisy.csv')\n",
        "  poly_features.to_csv(self.data_path+'poly_features_datanoisy.csv')\n",
        "  ZCR_features.to_csv(self.data_path+'ZCR_features_datanoisy.csv')\n",
        "  MFCCMSS_features.to_csv(self.data_path+'MFCCMSS_features_datanoisy.csv')\n",
        "  MFCCpoly_features.to_csv(self.data_path+'MFCCpoly_features_datanoisy.csv')\n",
        "  MFCCZCR_features.to_csv(self.data_path+'MFCCZCR_features_datanoisy.csv')\n",
        "  MSSpoly_feature.to_csv(self.data_path+'MSSpoly_feature_datanoisy.csv')\n",
        "  MSSZCR_features.to_csv(self.data_path+'MSSZCR_features_datanoisy.csv')\n",
        "  polyZCR_features.to_csv(self.data_path+'polyZCR_features_datanoisy.csv')\n",
        "  MFCCMSSpoly_features.to_csv(self.data_path+'MFCCMSSpoly_features_datanoisy.csv')\n",
        "  MFCCpolyZCR_features.to_csv(self.data_path+'MFCCpolyZCR_features_datanoisy.csv')\n",
        "  MSSpolyZCR_features.to_csv(self.data_path+'MSSpolyZCR_features_datanoisy.csv')\n",
        "  all_wave.to_csv(self.data_path+'all_wave_train_datanoisy.csv')\n",
        "  feature_all=np.array(Features_data['features'].tolist())\n",
        "  feature1=np.array(MFCC_features['features'].tolist())\n",
        "  feature2=np.array(MSS_features['features'].tolist())\n",
        "  feature3=np.array(poly_features['features'].tolist())\n",
        "  feature4=np.array(ZCR_features['features'].tolist())\n",
        "  feature_combo1=np.array(MFCCMSS_features['features'].tolist())\n",
        "  feature_combo2=np.array(MFCCpoly_features['features'].tolist())\n",
        "  feature_combo3=np.array(MFCCZCR_features['features'].tolist())\n",
        "  feature_combo4=np.array(MSSpoly_feature['features'].tolist())\n",
        "  feature_combo5=np.array(MSSZCR_features['features'].tolist())\n",
        "  feature_combo6=np.array(polyZCR_features['features'].tolist())\n",
        "  feature_combo7=np.array(MFCCMSSpoly_features['features'].tolist())\n",
        "  feature_combo8=np.array(MFCCpolyZCR_features['features'].tolist())\n",
        "  feature_combo9=np.array(MSSpolyZCR_features['features'].tolist())\n",
        "  target = Features_data.iloc[:,-1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bed\n",
            "bird\n",
            "cat\n",
            "down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAquaaFvGLuw"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # load data\n",
        "    x_tr, y_tr, x_t, y_t, x_val, y_val = loadData()\n",
        "\n",
        "    args.num_fc = 64\n",
        "    args.batch_size = 32\n",
        "    args.num_epochs = 1500  # best model will be saved before number of epochs reach this value\n",
        "    args.learning_rate = 0.0001\n",
        "    args.decay = 1e-6\n",
        "    args.momentum = 0.9\n",
        "\n",
        "    # define model\n",
        "    model = emo1d(input_shape=x_tr.shape[1:], num_classes=len(np.unique(np.argmax(y_tr, 1))), args=args)\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    model = train(model, x_tr, y_tr, x_val, y_val, args=args)\n",
        "\n",
        "    # test model\n",
        "    score = test(model, x_t, y_t) #[0.9742442428736396, 0.6445672231594283]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}